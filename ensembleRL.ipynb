{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/yhilpisch/tpqoa.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pylab import plt, mpl\n",
    "import tpqoa\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.float_format', '{:.5f}'.format)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('AUD/CAD', 'AUD_CAD'),\n",
       " ('AUD/CHF', 'AUD_CHF'),\n",
       " ('AUD/HKD', 'AUD_HKD'),\n",
       " ('AUD/JPY', 'AUD_JPY'),\n",
       " ('AUD/NZD', 'AUD_NZD')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/dylanyap/working_file/'\n",
    "api = tpqoa.tpqoa(path + 'crudential.cfg')\n",
    "ins = api.get_instruments()\n",
    "ins[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = api.get_history(instrument='GBP_JPY',\n",
    " start='2022.01.01',\n",
    " end='2022.12.25',\n",
    " granularity='M1',\n",
    " price='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363379, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>volume</th>\n",
       "      <th>complete</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:00:00</th>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:05:00</th>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:10:00</th>\n",
       "      <td>155.73700</td>\n",
       "      <td>155.74000</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:15:00</th>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:25:00</th>\n",
       "      <td>155.76900</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.73800</td>\n",
       "      <td>155.73900</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            o         h         l         c  volume  complete\n",
       "time                                                                         \n",
       "2022-01-02 22:00:00 155.75000 155.75000 155.75000 155.75000       1      True\n",
       "2022-01-02 22:05:00 155.73600 155.73600 155.71800 155.72500       6      True\n",
       "2022-01-02 22:10:00 155.73700 155.74000 155.71800 155.72500       6      True\n",
       "2022-01-02 22:15:00 155.74800 155.77500 155.74800 155.77500       4      True\n",
       "2022-01-02 22:25:00 155.76900 155.77500 155.73800 155.73900      12      True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.stream_data('GBP_JPY', stop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oandaenv as oe\n",
    "\n",
    "symbol = 'GBP_JPY'\n",
    "date = '2022-03-30'  \n",
    "features = [symbol, 'r', 's', 'm', 'v']\n",
    "\n",
    "learn_env = oe.OandaEnv(symbol=symbol,\n",
    "                  start=f'{date} 00:00:00',\n",
    "                  end=f'{date} 08:00:00',\n",
    "                  granularity='S30',\n",
    "                  price='M',\n",
    "                  features=features,\n",
    "                  window=20,\n",
    "                  lags=3,\n",
    "                  leverage=20,\n",
    "                  min_accuracy=0.4,\n",
    "                  min_performance=0.85\n",
    "                 )\n",
    "\n",
    "valid_env = oe.OandaEnv(symbol=learn_env.symbol,\n",
    "                  start=f'{date} 08:00:00',\n",
    "                  end=f'{date} 16:00:00',\n",
    "                  granularity=learn_env.granularity,\n",
    "                  price=learn_env.price,\n",
    "                  features=learn_env.features,\n",
    "                  window=learn_env.window,\n",
    "                  lags=learn_env.lags,\n",
    "                  leverage=learn_env.leverage,\n",
    "                  min_accuracy=0,\n",
    "                  min_performance=0,\n",
    "                  mu=learn_env.mu,\n",
    "                  std=learn_env.std\n",
    "                 )\n",
    "\n",
    "valid_env.data.info()\n",
    "\n",
    "test_env = oe.OandaEnv(symbol=learn_env.symbol,\n",
    "                  start=f'{date} 16:00:00',\n",
    "                  end=f'{date} 23:59:59',\n",
    "                  granularity=learn_env.granularity,\n",
    "                  price=learn_env.price,\n",
    "                  features=learn_env.features,\n",
    "                  window=learn_env.window,\n",
    "                  lags=learn_env.lags,\n",
    "                  leverage=learn_env.leverage,\n",
    "                  min_accuracy=0,\n",
    "                  min_performance=0,\n",
    "                  mu=learn_env.mu,\n",
    "                  std=learn_env.std\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data sections (learn, test, validate)\n",
    "ax = learn_env.data[learn_env.symbol].plot(figsize=(10, 6))\n",
    "plt.axvline(learn_env.data.index[-1], ls='-')\n",
    "valid_env.data[learn_env.symbol].plot(ax=ax, style='-.')\n",
    "plt.axvline(valid_env.data.index[-1], ls='-')\n",
    "test_env.data[learn_env.symbol].plot(ax=ax, style='-.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQL agent\n",
    "import tradingbot\n",
    "\n",
    "tradingbot.set_seeds(100)\n",
    "agent = tradingbot.TradingBot(24, 0.001, learn_env=learn_env,\n",
    "                              valid_env=valid_env)\n",
    "\n",
    "episodes = 31\n",
    "\n",
    "agent.learn(episodes)\n",
    "tradingbot.plot_performance(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2022-01-01'\n",
    "TRAIN_END_DATE = '2022-10-01'\n",
    "TEST_START_DATE = '2022-10-01'\n",
    "TEST_END_DATE = '2022-12-25'\n",
    "\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDICATORS = ['macd',\n",
    "              'rsi_30',\n",
    "              'cci_30',\n",
    "              'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>volume</th>\n",
       "      <th>complete</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:00:00</th>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:05:00</th>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:10:00</th>\n",
       "      <td>155.73700</td>\n",
       "      <td>155.74000</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:15:00</th>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-02 22:25:00</th>\n",
       "      <td>155.76900</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.73800</td>\n",
       "      <td>155.73900</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            o         h         l         c  volume  complete\n",
       "time                                                                         \n",
       "2022-01-02 22:00:00 155.75000 155.75000 155.75000 155.75000       1      True\n",
       "2022-01-02 22:05:00 155.73600 155.73600 155.71800 155.72500       6      True\n",
       "2022-01-02 22:10:00 155.73700 155.74000 155.71800 155.72500       6      True\n",
       "2022-01-02 22:15:00 155.74800 155.77500 155.74800 155.77500       4      True\n",
       "2022-01-02 22:25:00 155.76900 155.77500 155.73800 155.73900      12      True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.columns = [\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"tic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(raw)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-02 22:00:00</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>155.75000</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-66.66667</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-02 22:05:00</td>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.73600</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.00056</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-66.66667</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-02 22:10:00</td>\n",
       "      <td>155.73700</td>\n",
       "      <td>155.74000</td>\n",
       "      <td>155.71800</td>\n",
       "      <td>155.72500</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.00071</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-45.65217</td>\n",
       "      <td>76.42586</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-02 22:15:00</td>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.74800</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>68.15600</td>\n",
       "      <td>101.07527</td>\n",
       "      <td>16.08749</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-02 22:25:00</td>\n",
       "      <td>155.76900</td>\n",
       "      <td>155.77500</td>\n",
       "      <td>155.73800</td>\n",
       "      <td>155.73900</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>45.20694</td>\n",
       "      <td>31.77691</td>\n",
       "      <td>0.06214</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date      open      high       low     close  volume   tic  \\\n",
       "0 2022-01-02 22:00:00 155.75000 155.75000 155.75000 155.75000       1  True   \n",
       "1 2022-01-02 22:05:00 155.73600 155.73600 155.71800 155.72500       6  True   \n",
       "2 2022-01-02 22:10:00 155.73700 155.74000 155.71800 155.72500       6  True   \n",
       "3 2022-01-02 22:15:00 155.74800 155.77500 155.74800 155.77500       4  True   \n",
       "4 2022-01-02 22:25:00 155.76900 155.77500 155.73800 155.73900      12  True   \n",
       "\n",
       "      macd   rsi_30    cci_30     dx_30  turbulence  \n",
       "0  0.00000  0.00000 -66.66667 100.00000     0.00000  \n",
       "1 -0.00056  0.00000 -66.66667 100.00000     0.00000  \n",
       "2 -0.00071  0.00000 -45.65217  76.42586     0.00000  \n",
       "3  0.00104 68.15600 101.07527  16.08749     0.00000  \n",
       "4  0.00054 45.20694  31.77691   0.06214     0.00000  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 7\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebalance_window = 288 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 288 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=('2022-01-01','2022-10-01'),\n",
    "                 val_test_period=('2022-10-01','2022-12-25'),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-02T21:00:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_576_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.0023     |\n",
      "|    reward             | -0.00071312 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 2.46e-06    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 165      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 7.36e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 234      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00192 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 1.67e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000288 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 2.65e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 352      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00634  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 1.49e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 403       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000316 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 5.25e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 450      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00366  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 4.28e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 492      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.98    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00183  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.75     |\n",
      "|    value_loss         | 1.32e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 5.78e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.86     |\n",
      "|    value_loss         | 4.63e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 567      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00563  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.99     |\n",
      "|    value_loss         | 1.09e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 599      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000131 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 5.1e-09  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 629       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.24     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000237 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.28      |\n",
      "|    value_loss         | 1.93e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 657      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.64e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.44     |\n",
      "|    value_loss         | 6.68e-13 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 683      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00593  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.61     |\n",
      "|    value_loss         | 1.04e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 706      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 7.59e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.8      |\n",
      "|    value_loss         | 1.15e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 729       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -4.3e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3         |\n",
      "|    value_loss         | 5.46e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 750      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 2.46e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.21     |\n",
      "|    value_loss         | 1.23e-14 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 769       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.65     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -9.01e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.44      |\n",
      "|    value_loss         | 1.31e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 788      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.72    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 1.42e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.68     |\n",
      "|    value_loss         | 3.95e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 806      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -5.1e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.94     |\n",
      "|    value_loss         | 4.71e-14 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-02T21:00:00.000000000 to  2022-10-03T21:00:00.000000000\n",
      "A2C Sharpe Ratio:  0.0\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_576_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1923       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00402878 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1787        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518135 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.261      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00407    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | 0.027443087 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.0226      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1752        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011377742 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.636      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0105     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | -0.00035538 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.00947     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1721        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006007433 |\n",
      "|    clip_fraction        | 0.0592      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.259      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    reward               | 0.016003843 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.00678     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1713         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053289593 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.186       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0115      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    reward               | -0.0035484   |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.0042       |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-02T21:00:00.000000000 to  2022-10-03T21:00:00.000000000\n",
      "PPO Sharpe Ratio:  0.09579979895663296\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_576_1\n",
      "======DDPG Validation from:  2022-10-02T21:00:00.000000000 to  2022-10-03T21:00:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-03T21:00:00.000000000\n",
      "======Trading from:  2022-10-03T21:00:00.000000000 to  2022-10-04T21:00:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-03T21:00:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_864_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1354       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0454     |\n",
      "|    reward             | 0.04703408 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.00443    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1385         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.45        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.0444       |\n",
      "|    reward             | -0.048470233 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.00222      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1394        |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | 0.0229      |\n",
      "|    reward             | -0.00781074 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.000567    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1404        |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.0289     |\n",
      "|    reward             | -0.00535338 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.000274    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1408     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00175  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 2.43e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1411     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00267 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 2.9e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1414     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000756 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 4.89e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1417     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.59e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.17e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1419     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 7.8e-09  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 8.73e-17 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1420     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00128 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 7.12e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1422     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000986 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 6.07e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1423      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.00282  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 2.1e-06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1423     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00319  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 4.28e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1424     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000207 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 1.36e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1425     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00466 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.93     |\n",
      "|    value_loss         | 3.25e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1425     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000476 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 7.08e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1425      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000585 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 1e-07     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1426      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.94e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 1.78e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1426      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.43e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.54      |\n",
      "|    value_loss         | 1.4e-09   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1426      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -2.95e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.72      |\n",
      "|    value_loss         | 2.18e-10  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-03T21:00:00.000000000 to  2022-10-04T21:00:00.000000000\n",
      "A2C Sharpe Ratio:  -0.32167165554180277\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_864_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1974        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00124882 |\n",
      "------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1826          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.004578606   |\n",
      "|    clip_fraction        | 0.0271        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0185       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00114      |\n",
      "|    reward               | -0.0004906665 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.000932      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1779        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003414663 |\n",
      "|    clip_fraction        | 0.0178      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.000855    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1760         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062921382 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0282      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    reward               | -7.68186e-05 |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.000647     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1749         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125360405 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0214      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.944        |\n",
      "|    value_loss           | 0.000569     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-03T21:00:00.000000000 to  2022-10-04T21:00:00.000000000\n",
      "PPO Sharpe Ratio:  -0.9356332284739788\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_864_1\n",
      "======DDPG Validation from:  2022-10-03T21:00:00.000000000 to  2022-10-04T21:00:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-04T21:00:00.000000000\n",
      "======Trading from:  2022-10-04T21:00:00.000000000 to  2022-10-05T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-04T21:00:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_1152_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1291       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0489     |\n",
      "|    reward             | 0.04855628 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.00377    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1336         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.5         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.00329      |\n",
      "|    reward             | -0.002410888 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 5.31e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1334          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.52         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | -0.00143      |\n",
      "|    reward             | -0.0007513976 |\n",
      "|    std                | 1.11          |\n",
      "|    value_loss         | 1.2e-06       |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1355      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000545 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.61e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1367     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000783 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 7.12e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1375     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 8.3e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 4.9e-09  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00878  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 2.04e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1387     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00087  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 2.76e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1392     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00394 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 7.32e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1395      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.000451 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 7.18e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 1.09e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 5.24e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 9.21e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 3.15e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1397      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.00294  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 2.64e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00167  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 5.73e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1400      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000656 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.34      |\n",
      "|    value_loss         | 9.23e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000381 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 2.71e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1402      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000907 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.69      |\n",
      "|    value_loss         | 1.9e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 6.49e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.87     |\n",
      "|    value_loss         | 1.06e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.000264 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.08      |\n",
      "|    value_loss         | 1.73e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00236  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.3      |\n",
      "|    value_loss         | 1.24e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-04T21:00:00.000000000 to  2022-10-05T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.23446051635925938\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_1152_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1963       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -8.062e-05 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1812         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014267052 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0158      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 5.18e-06     |\n",
      "|    reward               | 0.026438966  |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.00052      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1768          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014177277   |\n",
      "|    clip_fraction        | 0.154         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.341        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0119       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00943      |\n",
      "|    reward               | -0.0012016158 |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 0.0119        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1752          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0051660603  |\n",
      "|    clip_fraction        | 0.0452        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.466         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0147       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00264      |\n",
      "|    reward               | -0.0004683457 |\n",
      "|    std                  | 0.984         |\n",
      "|    value_loss           | 0.0026        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1742          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010369396   |\n",
      "|    clip_fraction        | 0.134         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0272       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0108       |\n",
      "|    reward               | -0.0008751056 |\n",
      "|    std                  | 0.963         |\n",
      "|    value_loss           | 0.000202      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-04T21:00:00.000000000 to  2022-10-05T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2544735627199293\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1152_1\n",
      "======DDPG Validation from:  2022-10-04T21:00:00.000000000 to  2022-10-05T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-05T21:05:00.000000000\n",
      "======Trading from:  2022-10-05T21:05:00.000000000 to  2022-10-06T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-05T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_1440_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1338      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000491 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.49e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000198 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 7.18e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00251 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.25     |\n",
      "|    value_loss         | 2.9e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000159 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 9.72e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.0014  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.43     |\n",
      "|    value_loss         | 9.69e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1402     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00357 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.54     |\n",
      "|    value_loss         | 5.74e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 7.17e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.64     |\n",
      "|    value_loss         | 2.43e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1411     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00428  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.76     |\n",
      "|    value_loss         | 7.02e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1413     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00297 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.89     |\n",
      "|    value_loss         | 1.76e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1414     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.000567 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.02     |\n",
      "|    value_loss         | 8.35e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1417     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00459  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 3.28e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1417     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00108 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.32     |\n",
      "|    value_loss         | 3.85e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1418     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000386 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.49     |\n",
      "|    value_loss         | 3.02e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1416     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00257  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.66     |\n",
      "|    value_loss         | 1.28e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1417     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 7.08e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.86     |\n",
      "|    value_loss         | 1.24e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1418      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.000116 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 3.57e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1417      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -6.28e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.27      |\n",
      "|    value_loss         | 6.79e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1418     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.67    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00043  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 3.71e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1418     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 1.06e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.75     |\n",
      "|    value_loss         | 1.72e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1418      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -4.07e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.03      |\n",
      "|    value_loss         | 6.34e-16  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-05T21:05:00.000000000 to  2022-10-06T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.38131291538302187\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_1440_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1946         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0007358388 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1799         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019066851 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -7.99e-06    |\n",
      "|    reward               | 0.026119484  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.000194     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1756         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040898193 |\n",
      "|    clip_fraction        | 0.0136       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.538       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00884     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    reward               | 0.0001824048 |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.0176       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1736         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038621852 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.853       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.015       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    reward               | 0.01486399   |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.00713      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1725        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003658987 |\n",
      "|    clip_fraction        | 0.0297      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.782      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    reward               | -0.0034806  |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.00546     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-05T21:05:00.000000000 to  2022-10-06T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.3225202622136463\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1440_1\n",
      "======DDPG Validation from:  2022-10-05T21:05:00.000000000 to  2022-10-06T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-06T21:05:00.000000000\n",
      "======Trading from:  2022-10-06T21:05:00.000000000 to  2022-10-09T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-06T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_1728_1\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1356          |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 0             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.55         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | 0.003         |\n",
      "|    reward             | -0.0013056896 |\n",
      "|    std                | 1.14          |\n",
      "|    value_loss         | 6.7e-06       |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00194  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 2.81e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1393      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -2.37e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 3.8e-10   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1399     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000786 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 2.5e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1402      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -5.89e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 8.88e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1405     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.9     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00203  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.62     |\n",
      "|    value_loss         | 1.07e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1408     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.97    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00231  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.73     |\n",
      "|    value_loss         | 2.37e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1409      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.03     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000578 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.85      |\n",
      "|    value_loss         | 1.02e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1410      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000117 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.99      |\n",
      "|    value_loss         | 3.08e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00373 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 3.96e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1408     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.24    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00199  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.28     |\n",
      "|    value_loss         | 8.22e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1409     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 3.24e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.44     |\n",
      "|    value_loss         | 4.37e-17 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1409      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.78e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.61      |\n",
      "|    value_loss         | 9.81e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1410      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.45     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -3.92e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.79      |\n",
      "|    value_loss         | 2.63e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1411      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000745 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3         |\n",
      "|    value_loss         | 1.38e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1411      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.58     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 4.62e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.21      |\n",
      "|    value_loss         | 4.84e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1412      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -3.64e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.44      |\n",
      "|    value_loss         | 2.52e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1413      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.72     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.39e-08  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.68      |\n",
      "|    value_loss         | 1.34e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1413      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -6.09e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.93      |\n",
      "|    value_loss         | 5.33e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1414      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000776 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.2       |\n",
      "|    value_loss         | 1.14e-07  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-06T21:05:00.000000000 to  2022-10-09T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.162525137686639\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_1728_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1951       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00215588 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1808         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054945173 |\n",
      "|    clip_fraction        | 0.0614       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.872       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00851     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00396     |\n",
      "|    reward               | 0.001806594  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00524      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1765          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0024271253  |\n",
      "|    clip_fraction        | 0.0104        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.414        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0192       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000717     |\n",
      "|    reward               | -0.0004022668 |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 0.000639      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1745          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009835883   |\n",
      "|    clip_fraction        | 0.117         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0197       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0111       |\n",
      "|    reward               | -0.0005633364 |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 3.45e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1734          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.013725302   |\n",
      "|    clip_fraction        | 0.144         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0456       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0191       |\n",
      "|    reward               | -0.0006822816 |\n",
      "|    std                  | 0.938         |\n",
      "|    value_loss           | 2.48e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-06T21:05:00.000000000 to  2022-10-09T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0673470824959757\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1728_1\n",
      "======DDPG Validation from:  2022-10-06T21:05:00.000000000 to  2022-10-09T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-09T21:05:00.000000000\n",
      "======Trading from:  2022-10-09T21:05:00.000000000 to  2022-10-10T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-09T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_2016_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1338         |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.55        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 0.000846     |\n",
      "|    reward             | -1.57312e-05 |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 6.13e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00246 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 2.5e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1385      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000203 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 2e-08     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1394         |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.76        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -0.00278     |\n",
      "|    reward             | -3.14256e-05 |\n",
      "|    std                | 1.4          |\n",
      "|    value_loss         | 3.09e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1395     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00039 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.5      |\n",
      "|    value_loss         | 8.58e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.5e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.61     |\n",
      "|    value_loss         | 4.78e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1399      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.96     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.000212  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.72      |\n",
      "|    value_loss         | 2.02e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.03    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0075  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.84     |\n",
      "|    value_loss         | 7.38e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1402      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000401 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.96      |\n",
      "|    value_loss         | 2.88e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 4.4e-07  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.11     |\n",
      "|    value_loss         | 3.31e-14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000491 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.25     |\n",
      "|    value_loss         | 7.23e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1404      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -3.99e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.41      |\n",
      "|    value_loss         | 3.36e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.37     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 4.11e-06  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.59      |\n",
      "|    value_loss         | 2.84e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.44     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000414 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.77      |\n",
      "|    value_loss         | 2.43e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1406      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -2.47e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.96      |\n",
      "|    value_loss         | 1.27e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.004     |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.17      |\n",
      "|    value_loss         | 2.75e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1407      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -1.77e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.39      |\n",
      "|    value_loss         | 5.25e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 4.13e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.63     |\n",
      "|    value_loss         | 3.02e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000895 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.88     |\n",
      "|    value_loss         | 1.27e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00265  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.16     |\n",
      "|    value_loss         | 1.26e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-09T21:05:00.000000000 to  2022-10-10T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.23665846333951945\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_2016_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1943         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0028177567 |\n",
      "-------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1801          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0073923925  |\n",
      "|    clip_fraction        | 0.0858        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.515         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00564      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00619      |\n",
      "|    reward               | -0.0008188765 |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.0118        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1760          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005362657   |\n",
      "|    clip_fraction        | 0.0492        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.024        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00375      |\n",
      "|    reward               | -0.0001405898 |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 0.000339      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1741          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0095590735  |\n",
      "|    clip_fraction        | 0.0933        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0336       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00738      |\n",
      "|    reward               | -0.0001076682 |\n",
      "|    std                  | 0.971         |\n",
      "|    value_loss           | 0.000107      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1729        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013704475 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0577     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 9.24e-05    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-09T21:05:00.000000000 to  2022-10-10T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2224613920252136\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_2016_1\n",
      "======DDPG Validation from:  2022-10-09T21:05:00.000000000 to  2022-10-10T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-10T21:05:00.000000000\n",
      "======Trading from:  2022-10-10T21:05:00.000000000 to  2022-10-11T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-10T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_2304_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1332      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000226 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 3.82e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00233  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 3.63e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1385      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000389 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 8.16e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00212 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.41     |\n",
      "|    value_loss         | 1.56e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00284  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 3.79e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1394     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.9     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00121 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.61     |\n",
      "|    value_loss         | 5.5e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1396      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.000344  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.73      |\n",
      "|    value_loss         | 4.02e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1397      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 8.63e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.84      |\n",
      "|    value_loss         | 2.78e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00114 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.98     |\n",
      "|    value_loss         | 3.75e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.000929 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.12     |\n",
      "|    value_loss         | 1.5e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1399      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.00406  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.26      |\n",
      "|    value_loss         | 2.94e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1399      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000216 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.42      |\n",
      "|    value_loss         | 1.2e-08   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00357 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.59     |\n",
      "|    value_loss         | 2.34e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000979 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.77     |\n",
      "|    value_loss         | 2.55e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.51    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0014  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.96     |\n",
      "|    value_loss         | 3.84e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000124 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.18     |\n",
      "|    value_loss         | 3.26e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1402     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00433  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.41     |\n",
      "|    value_loss         | 3.48e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1402      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.71     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000524 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.65      |\n",
      "|    value_loss         | 4.48e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00413 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.92     |\n",
      "|    value_loss         | 3.03e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 4.44e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.2      |\n",
      "|    value_loss         | 3.43e-10 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-10T21:05:00.000000000 to  2022-10-11T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.23119824448683274\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_2304_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1936        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 8.95002e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1794         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037387204 |\n",
      "|    clip_fraction        | 0.0124       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00108     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    reward               | 0.000331798  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000589     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1752          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006594022   |\n",
      "|    clip_fraction        | 0.0569        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0198       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00272      |\n",
      "|    reward               | -0.0011926158 |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 0.000352      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009077487 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 0.000361    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1723          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008052701   |\n",
      "|    clip_fraction        | 0.087         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00383      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00965      |\n",
      "|    reward               | -0.0001075792 |\n",
      "|    std                  | 0.946         |\n",
      "|    value_loss           | 0.0002        |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-10T21:05:00.000000000 to  2022-10-11T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0876813544954775\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_2304_1\n",
      "======DDPG Validation from:  2022-10-10T21:05:00.000000000 to  2022-10-11T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-11T21:05:00.000000000\n",
      "======Trading from:  2022-10-11T21:05:00.000000000 to  2022-10-12T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-11T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_2592_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1281      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000219 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 3.37e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000157 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 1.53e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 8.53e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 2.36e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1378      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.00271  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.39      |\n",
      "|    value_loss         | 4.54e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1371      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -3.04e-10 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 3.33e-20  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.001   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 2.99e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1386      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.00123   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 4.61e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1355     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00088 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.82     |\n",
      "|    value_loss         | 2.21e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1360      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000736 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.95      |\n",
      "|    value_loss         | 1.03e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1364     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00141 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 7.4e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1367      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000783 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.24      |\n",
      "|    value_loss         | 1.89e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1370      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 1.72e-06  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.39      |\n",
      "|    value_loss         | 6.51e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1374     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 2.14e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.57     |\n",
      "|    value_loss         | 1.2e-12  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1363      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -1.42e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.75      |\n",
      "|    value_loss         | 4.18e-19  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1340      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -7.98e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.94      |\n",
      "|    value_loss         | 1.48e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 8.38e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.15     |\n",
      "|    value_loss         | 1.42e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 3.73e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.37     |\n",
      "|    value_loss         | 3.11e-16 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1344      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000153 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.61      |\n",
      "|    value_loss         | 4.27e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 5.72e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.87     |\n",
      "|    value_loss         | 1.71e-16 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1351     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 3.76e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.15     |\n",
      "|    value_loss         | 3.6e-14  |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-11T21:05:00.000000000 to  2022-10-12T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.24838874962828036\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_2592_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1955       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00235838 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1816         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044661677 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -2.22        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    reward               | 0.0017902225 |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.0033       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1767          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0033309902  |\n",
      "|    clip_fraction        | 0.00854       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0172       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000882     |\n",
      "|    reward               | -0.0005467848 |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 0.00111       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1747         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009371476  |\n",
      "|    clip_fraction        | 0.0987       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0108      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    reward               | 0.0002334814 |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 3.13e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012964115 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    reward               | -0.00156448 |\n",
      "|    std                  | 0.929       |\n",
      "|    value_loss           | 7.24e-06    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-11T21:05:00.000000000 to  2022-10-12T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.9621984112423104\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_2592_1\n",
      "======DDPG Validation from:  2022-10-11T21:05:00.000000000 to  2022-10-12T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-12T21:05:00.000000000\n",
      "======Trading from:  2022-10-12T21:05:00.000000000 to  2022-10-13T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-12T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_2880_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1349        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | -0.44       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.239       |\n",
      "|    reward             | 0.053008974 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.0258      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1381        |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | -1.11       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -0.0172     |\n",
      "|    reward             | -0.04967846 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.00318     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1390          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.46         |\n",
      "|    explained_variance | -1.54         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 0.0503        |\n",
      "|    reward             | -0.0077403993 |\n",
      "|    std                | 1.04          |\n",
      "|    value_loss         | 0.00189       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1393          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.47         |\n",
      "|    explained_variance | -6.65         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.0136       |\n",
      "|    reward             | -0.0050269994 |\n",
      "|    std                | 1.05          |\n",
      "|    value_loss         | 0.000262      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1398         |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.49        |\n",
      "|    explained_variance | -3.37        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 0.029        |\n",
      "|    reward             | -0.007406956 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 0.00296      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1402        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.51       |\n",
      "|    explained_variance | -2.38       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.029      |\n",
      "|    reward             | -0.04877889 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.000811    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1405       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | -16.6      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 0.0432     |\n",
      "|    reward             | -0.0043603 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.0044     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1406       |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | 0.318      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -0.0159    |\n",
      "|    reward             | 0.00649268 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.000247   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1407         |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.55        |\n",
      "|    explained_variance | -3.43        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | 0.116        |\n",
      "|    reward             | -0.029359596 |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 0.00844      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1407        |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -0.0522     |\n",
      "|    reward             | -0.01670871 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.000588    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1408       |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -0.0338    |\n",
      "|    reward             | 0.03533095 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.000283   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1408       |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.0275     |\n",
      "|    reward             | -0.0362378 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.000614   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1409     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0.481    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0809   |\n",
      "|    reward             | 0.007362 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 0.00308  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1409       |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.0382    |\n",
      "|    reward             | -0.0085834 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.000829   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1409     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0129  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 0.000413 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1410      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.62     |\n",
      "|    explained_variance | -7.7      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0461   |\n",
      "|    reward             | 0.0257334 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 0.00123   |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1410         |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.63        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | 0.0309       |\n",
      "|    reward             | -0.025411598 |\n",
      "|    std                | 1.24         |\n",
      "|    value_loss         | 0.000634     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1411      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.0294   |\n",
      "|    reward             | 0.0018369 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 0.000355  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1410       |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.0195    |\n",
      "|    reward             | -0.0385686 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 0.000446   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1410      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | -0.762    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.0702   |\n",
      "|    reward             | -0.104652 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 0.00208   |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-12T21:05:00.000000000 to  2022-10-13T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  0.14702128905365341\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_2880_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1942         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0036450592 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1805        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003966419 |\n",
      "|    clip_fraction        | 0.0168      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00792    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000635   |\n",
      "|    reward               | 0.024062207 |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.00132     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1766         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00500355   |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -5.04        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0237      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    reward               | 0.0004862204 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00989      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1746         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009138081 |\n",
      "|    clip_fraction        | 0.00386      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.167       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000363    |\n",
      "|    reward               | 0.01109241   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00463      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1734         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024249505 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00322     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0146      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000851    |\n",
      "|    reward               | -0.00444088  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00181      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-12T21:05:00.000000000 to  2022-10-13T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.057557459499094944\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_2880_1\n",
      "======DDPG Validation from:  2022-10-12T21:05:00.000000000 to  2022-10-13T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-13T21:05:00.000000000\n",
      "======Trading from:  2022-10-13T21:05:00.000000000 to  2022-10-16T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-13T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_3168_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1347        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.00348     |\n",
      "|    reward             | 0.001728216 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 6.67e-06    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1383     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 7.66e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1394     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00252 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 2.18e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000156 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 1.43e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00103 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 3.78e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00101  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 3.72e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00219 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 1.23e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1411     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0016  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.83     |\n",
      "|    value_loss         | 1.23e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1413     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.09    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00091 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.95     |\n",
      "|    value_loss         | 2.68e-07 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1415          |\n",
      "|    iterations         | 1000          |\n",
      "|    time_elapsed       | 3             |\n",
      "|    total_timesteps    | 5000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.16         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 999           |\n",
      "|    policy_loss        | -0.000114     |\n",
      "|    reward             | -0.0002923549 |\n",
      "|    std                | 2.09          |\n",
      "|    value_loss         | 5.05e-09      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1415     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000521 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.24     |\n",
      "|    value_loss         | 2.37e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1417     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00027 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.4      |\n",
      "|    value_loss         | 2.09e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1416      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.000544 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.58      |\n",
      "|    value_loss         | 6.75e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1415      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000545 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.75      |\n",
      "|    value_loss         | 5.5e-08   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1416      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -9.42e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.95      |\n",
      "|    value_loss         | 2.06e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1416     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0051  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.15     |\n",
      "|    value_loss         | 6.5e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1377     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 4.41e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.37     |\n",
      "|    value_loss         | 4.35e-17 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000337 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.61     |\n",
      "|    value_loss         | 1.53e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1383      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.02e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.86      |\n",
      "|    value_loss         | 8.39e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1384     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00192  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.14     |\n",
      "|    value_loss         | 3.78e-07 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-13T21:05:00.000000000 to  2022-10-16T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.26848418201337415\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_3168_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1954         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0006798644 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1813         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020514666 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0144      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000184     |\n",
      "|    reward               | 0.026256114  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00146      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1767         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011924764  |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.715       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00188     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00821     |\n",
      "|    reward               | 0.0010307692 |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.0425       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1745         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092415195 |\n",
      "|    clip_fraction        | 0.0957       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.411       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00581     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    reward               | 0.016492981  |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.02         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1733         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050715376 |\n",
      "|    clip_fraction        | 0.0422       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.883       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0228      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00647     |\n",
      "|    reward               | -0.0035112   |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 0.00639      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-13T21:05:00.000000000 to  2022-10-16T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.1538562884099797\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_3168_1\n",
      "======DDPG Validation from:  2022-10-13T21:05:00.000000000 to  2022-10-16T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-16T21:05:00.000000000\n",
      "======Trading from:  2022-10-16T21:05:00.000000000 to  2022-10-17T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-16T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_3456_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000306 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 6.71e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00196  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 2.69e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00491  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 1.08e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00186  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 1.19e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 7.93e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000401 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.58      |\n",
      "|    value_loss         | 7.26e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1406      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -2.73e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 2.45e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 9.37e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 2.77e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1408      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -3.47e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 4.74e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0025   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 1.77e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0039  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.22     |\n",
      "|    value_loss         | 4.6e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 3.36e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.37     |\n",
      "|    value_loss         | 3.48e-17 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1408      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -4.25e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.54      |\n",
      "|    value_loss         | 3.71e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1409     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000174 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.72     |\n",
      "|    value_loss         | 7.47e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 5.97e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.92     |\n",
      "|    value_loss         | 4.02e-16 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.00101 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.13     |\n",
      "|    value_loss         | 1.33e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.000227 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.35     |\n",
      "|    value_loss         | 7.04e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1410     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 2.59e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.59     |\n",
      "|    value_loss         | 1.12e-14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1411     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00614 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.85     |\n",
      "|    value_loss         | 7.79e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1411      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.84     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.00201  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.12      |\n",
      "|    value_loss         | 7.87e-07  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-16T21:05:00.000000000 to  2022-10-17T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.16681084342851776\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_3456_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1941         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0007985336 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1801         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064272685 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0113      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    reward               | -0.00092703  |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.000415     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1756         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00299434   |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0197      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000872    |\n",
      "|    reward               | -0.000737349 |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.000353     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1736          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008715189   |\n",
      "|    clip_fraction        | 0.073         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0318       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00597      |\n",
      "|    reward               | -0.0001888054 |\n",
      "|    std                  | 0.971         |\n",
      "|    value_loss           | 0.000172      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1724          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009826703   |\n",
      "|    clip_fraction        | 0.154         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0302       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0137       |\n",
      "|    reward               | -0.0014236768 |\n",
      "|    std                  | 0.948         |\n",
      "|    value_loss           | 0.000211      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-16T21:05:00.000000000 to  2022-10-17T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.9452133582370441\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_3456_1\n",
      "======DDPG Validation from:  2022-10-16T21:05:00.000000000 to  2022-10-17T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-17T21:05:00.000000000\n",
      "======Trading from:  2022-10-17T21:05:00.000000000 to  2022-10-18T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-17T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_3744_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1327        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.0234      |\n",
      "|    reward             | 0.027301252 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.00104     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1366       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -10.5      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.113     |\n",
      "|    reward             | -0.0500959 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0211     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1380       |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -0.825     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -0.0783    |\n",
      "|    reward             | -0.0069311 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0034     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1388       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.0409    |\n",
      "|    reward             | -0.0044058 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.000584   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1392         |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | -0.0367      |\n",
      "|    reward             | -0.007107346 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 0.000642     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1395      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.0129   |\n",
      "|    reward             | -0.048433 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.0002    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1396      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.072    |\n",
      "|    reward             | -0.004403 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.00145   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0115  |\n",
      "|    reward             | 0.008177 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.000274 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1399       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.137      |\n",
      "|    reward             | -0.0283005 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.00662    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1399       |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -0.00285   |\n",
      "|    reward             | -0.0157225 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 9.21e-05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0354  |\n",
      "|    reward             | 0.037728 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 0.000324 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1402       |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.0306     |\n",
      "|    reward             | -0.0370992 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.000961   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1400      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.0653    |\n",
      "|    reward             | 0.0075456 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 0.00307   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1400       |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.61      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.046     |\n",
      "|    reward             | -0.0088032 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 0.000859   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0233  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 0.000411 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1401      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0233   |\n",
      "|    reward             | 0.0264054 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 0.000316  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1401       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.038      |\n",
      "|    reward             | -0.0257767 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 0.000727   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1401      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.024    |\n",
      "|    reward             | 0.0018861 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 0.000402  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1401       |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.67      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.0324    |\n",
      "|    reward             | -0.0396081 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.000484   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1402       |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.0855    |\n",
      "|    reward             | -0.1075077 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 0.00246    |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-17T21:05:00.000000000 to  2022-10-18T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.1472518466166683\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_3744_1\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1943          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0002703768 |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1796        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002222474 |\n",
      "|    clip_fraction        | 0.00249     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.000961   |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000531   |\n",
      "|    reward               | 0.02571145  |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.000475    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1709         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063997144 |\n",
      "|    clip_fraction        | 0.0807       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.05        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    reward               | -0.001155544 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.0147       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1702        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002480186 |\n",
      "|    clip_fraction        | 0.017       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.0753     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.000842   |\n",
      "|    reward               | -0.00099031 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00115     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1635          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005268608   |\n",
      "|    clip_fraction        | 0.0434        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0196       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00465      |\n",
      "|    reward               | -0.0004902992 |\n",
      "|    std                  | 0.981         |\n",
      "|    value_loss           | 0.000271      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-17T21:05:00.000000000 to  2022-10-18T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.1710633111912923\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_3744_1\n",
      "======DDPG Validation from:  2022-10-17T21:05:00.000000000 to  2022-10-18T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-18T21:05:00.000000000\n",
      "======Trading from:  2022-10-18T21:05:00.000000000 to  2022-10-19T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-18T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_4032_1\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1323          |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 0             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | 0.000785      |\n",
      "|    reward             | -0.0001925424 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 6.01e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 7.09e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00187 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 1.01e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1350      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 0.000216  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.33e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1364     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00142 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 1.07e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.0012   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 5.66e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00179  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 1.36e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000956 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.75      |\n",
      "|    value_loss         | 4.46e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000338 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.87     |\n",
      "|    value_loss         | 4.58e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1384      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -3.76e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 3.7e-10   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1386     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00237 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 8.9e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1389     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.25    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00178 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.29     |\n",
      "|    value_loss         | 8.53e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00449 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.45     |\n",
      "|    value_loss         | 2.2e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1393      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000884 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.63      |\n",
      "|    value_loss         | 2.17e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1395     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 5.68e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.82     |\n",
      "|    value_loss         | 8.29e-12 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1397      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.00921   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.02      |\n",
      "|    value_loss         | 2.21e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00166  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.24     |\n",
      "|    value_loss         | 4.87e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1342     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.71e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.45     |\n",
      "|    value_loss         | 3.77e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1345      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.71e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.69      |\n",
      "|    value_loss         | 5.14e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1348      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.79     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.00513  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.95      |\n",
      "|    value_loss         | 3.74e-06  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-18T21:05:00.000000000 to  2022-10-19T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3884511463739202\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_4032_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1891         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.000400897 |\n",
      "-------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1768          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010442896   |\n",
      "|    clip_fraction        | 0.167         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.268         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0212       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0143       |\n",
      "|    reward               | -0.0014032965 |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 0.0332        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1737         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075161196 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0234      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    reward               | -0.00153658  |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 0.00078      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1722         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0121024195 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0148      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0127      |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 0.000388     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1706          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010167321   |\n",
      "|    clip_fraction        | 0.15          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0508       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0124       |\n",
      "|    reward               | -0.0001720928 |\n",
      "|    std                  | 0.931         |\n",
      "|    value_loss           | 0.000391      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-18T21:05:00.000000000 to  2022-10-19T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0995369321024653\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_4032_1\n",
      "======DDPG Validation from:  2022-10-18T21:05:00.000000000 to  2022-10-19T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-19T21:05:00.000000000\n",
      "======Trading from:  2022-10-19T21:05:00.000000000 to  2022-10-20T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-19T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_4320_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1251     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | -1.69    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0289  |\n",
      "|    reward             | 0.055083 |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 0.00788  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1331       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.02       |\n",
      "|    reward             | -0.0508837 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.00138    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1360       |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 0.024      |\n",
      "|    reward             | -0.0070224 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.000667   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1371       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -49.5      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.0637    |\n",
      "|    reward             | -0.0044618 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.00403    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1381       |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | 0.0204     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -0.0342    |\n",
      "|    reward             | -0.0070004 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.00104    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1388       |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.0118    |\n",
      "|    reward             | -0.0489797 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.000207   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1391       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.0495    |\n",
      "|    reward             | -0.0044527 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.00131    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1393      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.0106   |\n",
      "|    reward             | 0.0082693 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 0.000288  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1395       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.117      |\n",
      "|    reward             | -0.0286245 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.00568    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1397       |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -0.00765   |\n",
      "|    reward             | -0.0159025 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.0001     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    reward             | 0.038166 |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 0.000365 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1396       |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.6       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.018      |\n",
      "|    reward             | -0.0375299 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 0.00086    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1398      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.0828    |\n",
      "|    reward             | 0.0076332 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 0.0032    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1400       |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.036     |\n",
      "|    reward             | -0.0089054 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 0.000905   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1400     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0169  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 0.000416 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1401      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0257   |\n",
      "|    reward             | 0.0267162 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 0.000325  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1402       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.035      |\n",
      "|    reward             | -0.0260801 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 0.000775   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1403      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.0306   |\n",
      "|    reward             | 0.0019083 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 0.000442  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1403       |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.73      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.0268    |\n",
      "|    reward             | -0.0400743 |\n",
      "|    std                | 1.36       |\n",
      "|    value_loss         | 0.000521   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1404       |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.73      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.0923    |\n",
      "|    reward             | -0.1087731 |\n",
      "|    std                | 1.37       |\n",
      "|    value_loss         | 0.00254    |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-19T21:05:00.000000000 to  2022-10-20T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.0420682145243195\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_4320_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1929        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.004159518 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1799         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042876475 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.943       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00835     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00364     |\n",
      "|    reward               | -0.001019733 |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.00369      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1756         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027708758 |\n",
      "|    clip_fraction        | 0.0302       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.111       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    reward               | -0.00150298  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.000949     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1736        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009123085 |\n",
      "|    clip_fraction        | 0.0907      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0299     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00883    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 0.000141    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1725        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012000779 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0344     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.942       |\n",
      "|    value_loss           | 8.74e-05    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-19T21:05:00.000000000 to  2022-10-20T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0717975337093948\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_4320_1\n",
      "======DDPG Validation from:  2022-10-19T21:05:00.000000000 to  2022-10-20T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-20T21:05:00.000000000\n",
      "======Trading from:  2022-10-20T21:05:00.000000000 to  2022-10-23T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-20T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_4608_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1345     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000717 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 3.61e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1379     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00213 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 2.34e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1384          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.7          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | -0.00204      |\n",
      "|    reward             | -0.0008492256 |\n",
      "|    std                | 1.33          |\n",
      "|    value_loss         | 1.57e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1386     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 7e-05    |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 2.51e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1393     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000161 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 1.37e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1394     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00209 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 1.44e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1398      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000299 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.74      |\n",
      "|    value_loss         | 2.28e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1400      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000311 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.87      |\n",
      "|    value_loss         | 2.82e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000347 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 2.29e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00295 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 1.38e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1404      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.25     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -8.61e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.29      |\n",
      "|    value_loss         | 1.55e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00923  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.45     |\n",
      "|    value_loss         | 1.87e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.38     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 8.72e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.62      |\n",
      "|    value_loss         | 2.26e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.45     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -2.55e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.81      |\n",
      "|    value_loss         | 1.49e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1405     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000438 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.01     |\n",
      "|    value_loss         | 2.94e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1406     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00361  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.22     |\n",
      "|    value_loss         | 2.76e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.000301 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.45     |\n",
      "|    value_loss         | 1.83e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1407     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00225  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.7      |\n",
      "|    value_loss         | 8.7e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1408      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.28e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.96      |\n",
      "|    value_loss         | 1.89e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1408      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -8.74e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.23      |\n",
      "|    value_loss         | 1.62e-11  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-20T21:05:00.000000000 to  2022-10-23T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.24047066524912014\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_4608_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 1919      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.0015273 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1786         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037358187 |\n",
      "|    clip_fraction        | 0.0197       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0205      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    reward               | 0.00142135   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.000225     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1748         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060714595 |\n",
      "|    clip_fraction        | 0.0518       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00856     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00351     |\n",
      "|    reward               | -0.00077359  |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.000135     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1728          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0120774545  |\n",
      "|    clip_fraction        | 0.154         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0381       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.015        |\n",
      "|    reward               | -0.0006567201 |\n",
      "|    std                  | 0.952         |\n",
      "|    value_loss           | 9.06e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1716          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.013758325   |\n",
      "|    clip_fraction        | 0.15          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0201       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0208       |\n",
      "|    reward               | -0.0001095136 |\n",
      "|    std                  | 0.915         |\n",
      "|    value_loss           | 0.000277      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-20T21:05:00.000000000 to  2022-10-23T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.9794637714249081\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_4608_1\n",
      "======DDPG Validation from:  2022-10-20T21:05:00.000000000 to  2022-10-23T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-23T21:05:00.000000000\n",
      "======Trading from:  2022-10-23T21:05:00.000000000 to  2022-10-24T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-23T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_4896_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1339      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 0.0492    |\n",
      "|    reward             | 0.05504   |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.00465   |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1369         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.46        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.0231       |\n",
      "|    reward             | -0.050835222 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.00164      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1384         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | 0.0333       |\n",
      "|    reward             | -0.008223106 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.000788     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1391       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.0213    |\n",
      "|    reward             | -0.0044702 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.000361   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1393       |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -1.96      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -0.0266    |\n",
      "|    reward             | -0.0070224 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.000867   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1393       |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.0145    |\n",
      "|    reward             | -0.0491491 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.000234   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1396       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.51      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.0675    |\n",
      "|    reward             | -0.0044674 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.00139    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1397      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.0126   |\n",
      "|    reward             | 0.0082966 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.000285  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1322       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.0815     |\n",
      "|    reward             | -0.0287145 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.00522    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1331       |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | -1.69      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -0.0121    |\n",
      "|    reward             | -0.0159525 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.000266   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0353  |\n",
      "|    reward             | 0.03828  |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.00033  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1343       |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.0168     |\n",
      "|    reward             | -0.0376361 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.000909   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1349      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.0679    |\n",
      "|    reward             | 0.0076536 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 0.00301   |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1354        |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.6        |\n",
      "|    explained_variance | -71.3       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -0.122      |\n",
      "|    reward             | -0.00904023 |\n",
      "|    std                | 1.2         |\n",
      "|    value_loss         | 0.0122      |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1357     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | -0.887   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0171  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 0.000671 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1361     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    reward             | 0.026775 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 0.000344 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1364       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.0338     |\n",
      "|    reward             | -0.0261375 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.000987   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1367      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | 0.427     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.00871  |\n",
      "|    reward             | 0.0019125 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 7.76e-05  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1370       |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.66      |\n",
      "|    explained_variance | 3.88e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.0285    |\n",
      "|    reward             | -0.0401625 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 0.000598   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1372       |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.0792    |\n",
      "|    reward             | -0.1090125 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.00234    |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-23T21:05:00.000000000 to  2022-10-24T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.13396976122993504\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_4896_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1924       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00291728 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1791         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030576275 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.301       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.013       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    reward               | 0.02682295   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00157      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1749         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024918434 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -3.72        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00952     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00278     |\n",
      "|    reward               | 0.0006285024 |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.00838      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1729        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004329817 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.196      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    reward               | 0.013531247 |\n",
      "|    std                  | 0.982       |\n",
      "|    value_loss           | 0.00655     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1718         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045648627 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.0933      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0238      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00341     |\n",
      "|    reward               | -0.00194548  |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 0.0042       |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-23T21:05:00.000000000 to  2022-10-24T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.36761106721583287\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_4896_1\n",
      "======DDPG Validation from:  2022-10-23T21:05:00.000000000 to  2022-10-24T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-24T21:05:00.000000000\n",
      "======Trading from:  2022-10-24T21:05:00.000000000 to  2022-10-25T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-24T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_5184_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1332       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0322     |\n",
      "|    reward             | 0.02202213 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.000905   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1373       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.0221     |\n",
      "|    reward             | -0.0135841 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.000264   |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1385          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.54         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | -0.00142      |\n",
      "|    reward             | -0.0011105424 |\n",
      "|    std                | 1.13          |\n",
      "|    value_loss         | 1.28e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1393          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.57         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | 1.85e-06      |\n",
      "|    reward             | -0.0013198752 |\n",
      "|    std                | 1.17          |\n",
      "|    value_loss         | 1.22e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00158 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 1.07e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1391      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.73e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.87e-18  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1396      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.000306  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.38      |\n",
      "|    value_loss         | 4.17e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00205  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 1.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000821 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.58     |\n",
      "|    value_loss         | 2.71e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00252  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.69     |\n",
      "|    value_loss         | 2.29e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1400      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.01     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.000893  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.81      |\n",
      "|    value_loss         | 2.49e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1401     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00129  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.94     |\n",
      "|    value_loss         | 4.52e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1402      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.000476 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.08      |\n",
      "|    value_loss         | 6.82e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1403     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00047 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.23     |\n",
      "|    value_loss         | 7.7e-08  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1403      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000136 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.39      |\n",
      "|    value_loss         | 5.15e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1404      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -3.42e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.55      |\n",
      "|    value_loss         | 2.59e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1404      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -6.49e-11 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.73      |\n",
      "|    value_loss         | 9.25e-22  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1404     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 2.85e-10 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.93     |\n",
      "|    value_loss         | 2.31e-20 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1405     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000108 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.14     |\n",
      "|    value_loss         | 3.11e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1405      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000172 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.36      |\n",
      "|    value_loss         | 5e-09     |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-24T21:05:00.000000000 to  2022-10-25T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.28921413216726005\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_5184_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 1912      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 4.808e-05 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1781         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045140167 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    reward               | 0.028315313  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.000511     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1740         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006835704  |\n",
      "|    clip_fraction        | 0.0837       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.72        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00732     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0071      |\n",
      "|    reward               | 0.0003882818 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0227       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1721         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025121013 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.28        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00535     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    reward               | 0.0014942721 |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1710         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042443564 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.651        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    reward               | -9.38688e-05 |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.000705     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-24T21:05:00.000000000 to  2022-10-25T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3888736481309591\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_5184_1\n",
      "======DDPG Validation from:  2022-10-24T21:05:00.000000000 to  2022-10-25T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-25T21:05:00.000000000\n",
      "======Trading from:  2022-10-25T21:05:00.000000000 to  2022-10-26T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-25T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_5472_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1330       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | -8.76      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -0.0785    |\n",
      "|    reward             | 0.05465783 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0284     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1368      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | 0.0656    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.0498   |\n",
      "|    reward             | -0.049725 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.00198   |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1377         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.46        |\n",
      "|    explained_variance | -102         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | -0.114       |\n",
      "|    reward             | -0.007878105 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 0.0222       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1382       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -439       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.157     |\n",
      "|    reward             | -0.0044233 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.021      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1388         |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | -7.25        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 0.412        |\n",
      "|    reward             | -0.007957481 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.0418       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1390        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | -17.3       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 0.126       |\n",
      "|    reward             | -0.04879687 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.0157      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1391       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.0486    |\n",
      "|    reward             | -0.0055371 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.00184    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1392       |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -1.96      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.0107     |\n",
      "|    reward             | 0.00569708 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.000456   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1394       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -7.39      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.116      |\n",
      "|    reward             | -0.0274872 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.00697    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1395         |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.49        |\n",
      "|    explained_variance | -12.7        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 0.00858      |\n",
      "|    reward             | -0.015404081 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 0.0013       |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | -32.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00593 |\n",
      "|    reward             | 0.036792 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 0.00112  |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1396         |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | -1.64        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | 0.0299       |\n",
      "|    reward             | -0.036088478 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 0.0021       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1396      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | -9.06     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.0297    |\n",
      "|    reward             | 0.0072912 |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 0.0026    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1397        |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | 0.275       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -0.0322     |\n",
      "|    reward             | -0.00788729 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.000604    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1397        |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -0.0131     |\n",
      "|    reward             | -0.00155738 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.000353    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00164  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 2.49e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 6.64e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00118  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 8.85e-07 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1398        |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 0.00116     |\n",
      "|    reward             | -0.00156877 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 8.81e-07    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1398     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 7.75e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 2.69e-09 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-25T21:05:00.000000000 to  2022-10-26T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.6052281378284614\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_5472_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1906         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0012134336 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1769         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011035887 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0829      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000111     |\n",
      "|    reward               | 0.0005416405 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00107      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005300201 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    reward               | 0.0001786   |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 0.00061     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1716        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010111911 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00769    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.000394    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1706         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.011944314  |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.018       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0131      |\n",
      "|    reward               | -5.16896e-05 |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 0.00046      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-25T21:05:00.000000000 to  2022-10-26T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3331053518833296\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_5472_1\n",
      "======DDPG Validation from:  2022-10-25T21:05:00.000000000 to  2022-10-26T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-26T21:05:00.000000000\n",
      "======Trading from:  2022-10-26T21:05:00.000000000 to  2022-10-27T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-26T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_5760_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1320       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -3.8       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.221      |\n",
      "|    reward             | 0.05185868 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.033      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1360       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -11.9      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.0581    |\n",
      "|    reward             | -0.0507355 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0185     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1375        |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | -15.6       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | 0.17        |\n",
      "|    reward             | -0.00843114 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.0112      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1381          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.48         |\n",
      "|    explained_variance | -3.32         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.0127       |\n",
      "|    reward             | -0.0051887296 |\n",
      "|    std                | 1.07          |\n",
      "|    value_loss         | 8.69e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1386          |\n",
      "|    iterations         | 500           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.49         |\n",
      "|    explained_variance | -0.786        |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 499           |\n",
      "|    policy_loss        | -0.0266       |\n",
      "|    reward             | -0.0073986975 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 0.000814      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1388        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.5        |\n",
      "|    explained_variance | -2.55       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.0237     |\n",
      "|    reward             | -0.04867066 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.000751    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1388         |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.52        |\n",
      "|    explained_variance | 0.196        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -0.128       |\n",
      "|    reward             | -0.005701927 |\n",
      "|    std                | 1.11         |\n",
      "|    value_loss         | 0.011        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1388       |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | -0.0592    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -0.0216    |\n",
      "|    reward             | 0.00621838 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.000467   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1389         |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54        |\n",
      "|    explained_variance | -16.7        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | 0.0659       |\n",
      "|    reward             | -0.027871732 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 0.00441      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1389        |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | -163        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 0.0706      |\n",
      "|    reward             | -0.01634371 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.0188      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1389       |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | -8.7       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 0.0061     |\n",
      "|    reward             | 0.03442286 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.000294   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1391        |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 0.0191      |\n",
      "|    reward             | -0.03620968 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.000743    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1391         |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.57        |\n",
      "|    explained_variance | -0.162       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 0.0857       |\n",
      "|    reward             | 0.0068409694 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 0.00236      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1392         |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 5            |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.58        |\n",
      "|    explained_variance | -9.59        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | -0.0951      |\n",
      "|    reward             | -0.008590776 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 0.00439      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1393        |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.6        |\n",
      "|    explained_variance | 0.183       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -0.159      |\n",
      "|    reward             | -0.00155738 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 0.0102      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1394      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.61     |\n",
      "|    explained_variance | -63.3     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.0306   |\n",
      "|    reward             | 0.0250614 |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 0.00529   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1394       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.61      |\n",
      "|    explained_variance | -4.95      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.0123     |\n",
      "|    reward             | -0.0244483 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 0.00068    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1395      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.0166   |\n",
      "|    reward             | 0.0017883 |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 0.000226  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1395         |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.62        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | -0.0238      |\n",
      "|    reward             | -0.038429227 |\n",
      "|    std                | 1.22         |\n",
      "|    value_loss         | 0.000386     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1395       |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.64      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.0636    |\n",
      "|    reward             | -0.1018647 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 0.00201    |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-26T21:05:00.000000000 to  2022-10-27T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.17758454684542838\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_5760_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1899         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0011109052 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1757         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028059115 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0198      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000947    |\n",
      "|    reward               | 0.001769374  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.000359     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1723          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010904946   |\n",
      "|    clip_fraction        | 0.127         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0231       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0114       |\n",
      "|    reward               | -0.0011562568 |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 5.27e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1708          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0118197035  |\n",
      "|    clip_fraction        | 0.146         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0444       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0171       |\n",
      "|    reward               | -0.0003731395 |\n",
      "|    std                  | 0.946         |\n",
      "|    value_loss           | 7.67e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1700          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014534504   |\n",
      "|    clip_fraction        | 0.16          |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.34         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0631       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0229       |\n",
      "|    reward               | -0.0006660368 |\n",
      "|    std                  | 0.907         |\n",
      "|    value_loss           | 2.73e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-26T21:05:00.000000000 to  2022-10-27T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0648091359979865\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_5760_1\n",
      "======DDPG Validation from:  2022-10-26T21:05:00.000000000 to  2022-10-27T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-27T21:05:00.000000000\n",
      "======Trading from:  2022-10-27T21:05:00.000000000 to  2022-10-30T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-27T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_6048_1\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1314          |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 0             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.52         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -0.00248      |\n",
      "|    reward             | -0.0010539904 |\n",
      "|    std                | 1.11          |\n",
      "|    value_loss         | 3.24e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1358     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0031   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 4.64e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -2.97e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 3.19e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00439 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 2.82e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1385     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00157 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 9.65e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1389      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000277 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 3.46e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1389     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00266  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 3.22e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1390      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 0.000111  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 5.57e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1392     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000158 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.91     |\n",
      "|    value_loss         | 7.19e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1394      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -2.68e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 2.15e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1395      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000739 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 1.74e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.000279 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.34     |\n",
      "|    value_loss         | 1.61e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.0029   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.51     |\n",
      "|    value_loss         | 5.77e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 7.33e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.69     |\n",
      "|    value_loss         | 1.41e-15 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000331 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.88     |\n",
      "|    value_loss         | 2.72e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1397     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00128  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.08     |\n",
      "|    value_loss         | 3.82e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1395      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000113 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.31      |\n",
      "|    value_loss         | 2.36e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.68    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00226 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.54     |\n",
      "|    value_loss         | 8.02e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1396     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00112  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.8      |\n",
      "|    value_loss         | 2.78e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1396      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 7.31e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.06      |\n",
      "|    value_loss         | 1.04e-13  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-27T21:05:00.000000000 to  2022-10-30T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.5752393942906541\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_6048_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1912       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00330248 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1773         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053628786 |\n",
      "|    clip_fraction        | 0.0919       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.21        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0286      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00631     |\n",
      "|    reward               | 0.00150775   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00435      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1730         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014126671 |\n",
      "|    clip_fraction        | 0.00488      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0215       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0246      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000243    |\n",
      "|    reward               | 9.6102e-06   |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 0.00109      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1707          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007719971   |\n",
      "|    clip_fraction        | 0.0648        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0184       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0059       |\n",
      "|    reward               | -0.0012675069 |\n",
      "|    std                  | 0.96          |\n",
      "|    value_loss           | 6.77e-05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1697        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008973286 |\n",
      "|    clip_fraction        | 0.0856      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0339     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00762    |\n",
      "|    reward               | -0.00162448 |\n",
      "|    std                  | 0.94        |\n",
      "|    value_loss           | 3.07e-05    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-10-27T21:05:00.000000000 to  2022-10-30T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -1.301550010574335\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_6048_1\n",
      "======DDPG Validation from:  2022-10-27T21:05:00.000000000 to  2022-10-30T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-30T21:05:00.000000000\n",
      "======Trading from:  2022-10-30T21:05:00.000000000 to  2022-10-31T21:05:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-30T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_6336_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1289      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000582 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.45e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1337      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.000534  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.05e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1355     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.000691 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 3.65e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 1.31e-09 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 1.2e-18  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1359     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 6.8e-08  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 5.61e-15 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00332 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 3.52e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1370      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000117 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.39      |\n",
      "|    value_loss         | 5.77e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1363      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000419 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 8.78e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1359     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00149 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 6.5e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1363     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 2.96e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 1.95e-12 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1365          |\n",
      "|    iterations         | 1100          |\n",
      "|    time_elapsed       | 4             |\n",
      "|    total_timesteps    | 5500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.02         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1099          |\n",
      "|    policy_loss        | 5.38e-05      |\n",
      "|    reward             | -0.0009424805 |\n",
      "|    std                | 1.83          |\n",
      "|    value_loss         | 9.84e-10      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00341 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.96     |\n",
      "|    value_loss         | 4.06e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1372      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.39e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 7.41e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1374     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.98e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.24     |\n",
      "|    value_loss         | 9.19e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1376      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000453 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.41      |\n",
      "|    value_loss         | 4.35e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1377     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.57     |\n",
      "|    value_loss         | 2.55e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1378      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.00155   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.76      |\n",
      "|    value_loss         | 4.42e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00316  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.96     |\n",
      "|    value_loss         | 1.4e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.00246   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.17      |\n",
      "|    value_loss         | 1.44e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1376     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -8.5e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.39     |\n",
      "|    value_loss         | 3.37e-15 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-10-30T21:05:00.000000000 to  2022-10-31T21:05:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2987178975271577\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_6336_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1885         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0025479435 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1758         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057488717 |\n",
      "|    clip_fraction        | 0.0429       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0328      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    reward               | 0.028988235  |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.000305     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1716         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042724432 |\n",
      "|    clip_fraction        | 0.0184       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -3.37        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00225     |\n",
      "|    reward               | 0.0007317178 |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.0196       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1683        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005342911 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.277      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    reward               | 0.016491564 |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.00672     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1676          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.004824031   |\n",
      "|    clip_fraction        | 0.0269        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.263        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0147       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00337      |\n",
      "|    reward               | -0.0048977216 |\n",
      "|    std                  | 0.988         |\n",
      "|    value_loss           | 0.00414       |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-10-30T21:05:00.000000000 to  2022-10-31T21:05:00.000000000\n",
      "PPO Sharpe Ratio:  -0.32779239155472734\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_6336_1\n",
      "======DDPG Validation from:  2022-10-30T21:05:00.000000000 to  2022-10-31T21:05:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-10-31T21:05:00.000000000\n",
      "======Trading from:  2022-10-31T21:05:00.000000000 to  2022-11-01T21:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-10-31T21:05:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_6624_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1322      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000157 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.28e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1358     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 4.26e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 9.15e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.000214 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 2.95e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1377     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00199 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 2.13e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00265  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 1.46e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1385     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00261 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 1.1e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1388     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000274 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 2.89e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00372 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 4.91e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1384     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000101 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 4.11e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1386     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00138  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 6.88e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1388     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00219 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.04     |\n",
      "|    value_loss         | 1.94e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1388      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.000476  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 5.69e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1389     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000171 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.34     |\n",
      "|    value_loss         | 6.75e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1390      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.34     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000625 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.51      |\n",
      "|    value_loss         | 3.08e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1391      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -5.78e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 8.26e-16  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1391     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 7.46e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.87     |\n",
      "|    value_loss         | 1.41e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1392     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 6.19e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 4.63e-16 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1393      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000383 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.28      |\n",
      "|    value_loss         | 2.4e-08   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1394     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00544 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 4.28e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1394      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000278 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.75      |\n",
      "|    value_loss         | 1.33e-08  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-10-31T21:05:00.000000000 to  2022-11-01T21:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.28310693049461466\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_6624_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1895        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.001314967 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1763        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006176709 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    reward               | 0.00793739  |\n",
      "|    std                  | 0.983       |\n",
      "|    value_loss           | 0.000504    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1725         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032927725 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.000597     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1705         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029402804 |\n",
      "|    clip_fraction        | 0.00112      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0068       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.000108     |\n",
      "|    reward               | 0.0008784845 |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 0.000838     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1695         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031462493 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00353     |\n",
      "|    reward               | -0.00160408  |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 0.000229     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-31T21:05:00.000000000 to  2022-11-01T21:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3244794383988028\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_6624_1\n",
      "======DDPG Validation from:  2022-10-31T21:05:00.000000000 to  2022-11-01T21:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-01T21:10:00.000000000\n",
      "======Trading from:  2022-11-01T21:10:00.000000000 to  2022-11-02T21:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-01T21:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_6912_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1297        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | -3.11       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.038      |\n",
      "|    reward             | 0.053881552 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.0121      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1340       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | -0.354     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.00437   |\n",
      "|    reward             | -0.0491911 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.00186    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1359         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | -72.4        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | 0.285        |\n",
      "|    reward             | -0.007901701 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.049        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1364         |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.49        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -0.00147     |\n",
      "|    reward             | -0.001314468 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 1.38e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1368          |\n",
      "|    iterations         | 500           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 499           |\n",
      "|    policy_loss        | 0.000792      |\n",
      "|    reward             | -0.0006682245 |\n",
      "|    std                | 1.09          |\n",
      "|    value_loss         | 1.36e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00205  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 4.25e-06 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1373        |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -0.00228    |\n",
      "|    reward             | -6.2388e-05 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 1.13e-06    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1363        |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.61       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -0.000169   |\n",
      "|    reward             | -0.00038858 |\n",
      "|    std                | 1.21        |\n",
      "|    value_loss         | 3.4e-07     |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.85e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 2.35e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1367     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 6.52e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 2.24e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1369      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.00186  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 1.35e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00729  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 2.06e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1372      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -2.66e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 1.24e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1374     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00249 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 1.59e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000665 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 1.58e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1376     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00501  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 5e-06    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1377     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 4.76e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 1.7e-16  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1378      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 6.76e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 1.53e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1379     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00125  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.53     |\n",
      "|    value_loss         | 2.62e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.00119 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.71     |\n",
      "|    value_loss         | 2.95e-07 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-01T21:10:00.000000000 to  2022-11-02T21:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2532463391836974\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_6912_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1884        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.000539631 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002587866 |\n",
      "|    clip_fraction        | 0.00894     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000383   |\n",
      "|    reward               | 0.027805932 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 0.000419    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1714         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037483845 |\n",
      "|    clip_fraction        | 0.00679      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.18         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0125      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000545    |\n",
      "|    reward               | 0.0010366974 |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.0017       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1697        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005957488 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.05       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    reward               | 0.01532219  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00708     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1686        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005764253 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.17       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0251     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    reward               | -0.00500008 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.00431     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-01T21:10:00.000000000 to  2022-11-02T21:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.3399001648838846\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_6912_1\n",
      "======DDPG Validation from:  2022-11-01T21:10:00.000000000 to  2022-11-02T21:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-02T21:10:00.000000000\n",
      "======Trading from:  2022-11-02T21:10:00.000000000 to  2022-11-03T21:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-02T21:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_7200_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00588  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 6.17e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00167 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 1.96e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1362     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00165  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 2.23e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00543 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 1.25e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1373     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00441  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.44     |\n",
      "|    value_loss         | 3.29e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1377     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -8.3e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.54     |\n",
      "|    value_loss         | 3.55e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1380     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00148 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.65     |\n",
      "|    value_loss         | 5.62e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1380      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000558 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 1.03e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00202 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.89     |\n",
      "|    value_loss         | 6.24e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1381     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.000382 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.02     |\n",
      "|    value_loss         | 4.78e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000223 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 5.07e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 8.48e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.32     |\n",
      "|    value_loss         | 1.05e-15 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000685 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.49     |\n",
      "|    value_loss         | 8.56e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1383     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.001    |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.67     |\n",
      "|    value_loss         | 2.97e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1383      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -2.78e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.85      |\n",
      "|    value_loss         | 1.46e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000664 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.06     |\n",
      "|    value_loss         | 7.97e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1382     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00191  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.28     |\n",
      "|    value_loss         | 6.5e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1383     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0032  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 1.96e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1384      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.03e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.77      |\n",
      "|    value_loss         | 1.57e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1384     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 6.67e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.02     |\n",
      "|    value_loss         | 1.25e-16 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-02T21:10:00.000000000 to  2022-11-03T21:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3259844754927121\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_7200_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1881         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0011674822 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1754         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044836327 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0314      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    reward               | 0.02232535   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.000817     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1714          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.011279895   |\n",
      "|    clip_fraction        | 0.113         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.822        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0143       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0114       |\n",
      "|    reward               | -0.0005791284 |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 0.0308        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1695         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004731719  |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.43         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0152      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    reward               | 0.0020287698 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00127      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1683         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034860661 |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00999     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00103     |\n",
      "|    reward               | -0.00163888  |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 0.000373     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-11-02T21:10:00.000000000 to  2022-11-03T21:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.4340393177978052\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_7200_1\n",
      "======DDPG Validation from:  2022-11-02T21:10:00.000000000 to  2022-11-03T21:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-03T21:10:00.000000000\n",
      "======Trading from:  2022-11-03T21:10:00.000000000 to  2022-11-06T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-03T21:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_7488_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1318        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.00128     |\n",
      "|    reward             | 4.84704e-05 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.86e-06    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1344         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.52        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | -0.000126    |\n",
      "|    reward             | -0.000218744 |\n",
      "|    std                | 1.11         |\n",
      "|    value_loss         | 1.49e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1358     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00215 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 1.96e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1364     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00162  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 1.73e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1365     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00123 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 4.67e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.002   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 1.87e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000285 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 3.41e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00152 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 9.76e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000825 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 2.29e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00172 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 4.18e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1373     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00447  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.04     |\n",
      "|    value_loss         | 5.99e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1373     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 3.25e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 2.88e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1374     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00482  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.34     |\n",
      "|    value_loss         | 2.41e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.00126  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.5       |\n",
      "|    value_loss         | 3.94e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1376     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000963 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.69     |\n",
      "|    value_loss         | 1.98e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1376     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.00955 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.88     |\n",
      "|    value_loss         | 7.99e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1377      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -8.48e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.08      |\n",
      "|    value_loss         | 1.25e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1377      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -5.48e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.3       |\n",
      "|    value_loss         | 5.23e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1378     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.68    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00137  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.53     |\n",
      "|    value_loss         | 3.56e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1375      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.75     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -3.41e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.79      |\n",
      "|    value_loss         | 1.42e-10  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-03T21:10:00.000000000 to  2022-11-06T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.253776444156652\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_7488_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1877       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00029288 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1746         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017015936 |\n",
      "|    clip_fraction        | 0.00747      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0148      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000158    |\n",
      "|    reward               | 0.02715415   |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.000464     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1708       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00823483 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.357      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.0259    |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    reward               | 0.001207   |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 0.0192     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1689         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044576456 |\n",
      "|    clip_fraction        | 0.0537       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.496       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0162      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00669     |\n",
      "|    reward               | 0.015277514  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00977      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1677          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0035535241  |\n",
      "|    clip_fraction        | 0.0288        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.343        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00982      |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00231      |\n",
      "|    reward               | -0.0038824752 |\n",
      "|    std                  | 0.981         |\n",
      "|    value_loss           | 0.00531       |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-03T21:10:00.000000000 to  2022-11-06T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.10825437956351355\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_7488_1\n",
      "======DDPG Validation from:  2022-11-03T21:10:00.000000000 to  2022-11-06T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-06T22:10:00.000000000\n",
      "======Trading from:  2022-11-06T22:10:00.000000000 to  2022-11-07T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-06T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_7776_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1298         |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.46        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 0.00407      |\n",
      "|    reward             | 0.0002368192 |\n",
      "|    std                | 1.04         |\n",
      "|    value_loss         | 8.79e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1344         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | -0.00187     |\n",
      "|    reward             | -0.000172073 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 2.54e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1356          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.57         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 0.00371       |\n",
      "|    reward             | -0.0005346976 |\n",
      "|    std                | 1.16          |\n",
      "|    value_loss         | 4.34e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1357     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000259 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 2.63e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1362     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00061 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 2.21e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1359      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.00442   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.42      |\n",
      "|    value_loss         | 9.7e-06   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1363     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00103  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 4.86e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1362      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.00127  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 4.38e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1365     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00135  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.74     |\n",
      "|    value_loss         | 5.34e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1368      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.04     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -3.56e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.86      |\n",
      "|    value_loss         | 3.73e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00646 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.99     |\n",
      "|    value_loss         | 1.2e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00073 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 9.73e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.25    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.29e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.29     |\n",
      "|    value_loss         | 3.13e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.32    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.84e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.46     |\n",
      "|    value_loss         | 1.03e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1372     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00262 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.63     |\n",
      "|    value_loss         | 1.75e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1374     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 8.28e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.81     |\n",
      "|    value_loss         | 1.38e-13 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1375     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 4.84e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.01     |\n",
      "|    value_loss         | 3.23e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1376     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000942 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.23     |\n",
      "|    value_loss         | 1.92e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1373     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00438 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.45     |\n",
      "|    value_loss         | 2.48e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1373     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.96e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.69     |\n",
      "|    value_loss         | 8.81e-11 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-06T22:10:00.000000000 to  2022-11-07T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.20759694474168777\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_7776_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1848        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.002246277 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1737         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010466686 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.014       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000409     |\n",
      "|    reward               | 0.02655415   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.000279     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1702          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007008491   |\n",
      "|    clip_fraction        | 0.0792        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -1.11         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0216       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00702      |\n",
      "|    reward               | -0.0002307488 |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 0.018         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1685          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073384726 |\n",
      "|    clip_fraction        | 0.00405       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -0.114        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0161       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000906     |\n",
      "|    reward               | 0.0007570256  |\n",
      "|    std                  | 0.973         |\n",
      "|    value_loss           | 0.00153       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1671         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024289854 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0235      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    reward               | -0.000993216 |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 0.000157     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-11-06T22:10:00.000000000 to  2022-11-07T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0182868637559297\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_7776_1\n",
      "======DDPG Validation from:  2022-11-06T22:10:00.000000000 to  2022-11-07T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-07T22:10:00.000000000\n",
      "======Trading from:  2022-11-07T22:10:00.000000000 to  2022-11-08T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-07T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_8064_1\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1292          |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 0             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.51         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -0.000642     |\n",
      "|    reward             | -0.0002292416 |\n",
      "|    std                | 1.1           |\n",
      "|    value_loss         | 6.48e-07      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1335        |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.58       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 0.00466     |\n",
      "|    reward             | -0.00070329 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 1.52e-05    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00142 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 8.75e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1355     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00205 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 1.66e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1360      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000898 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 3.85e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1362     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00316  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 1.97e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1363     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 4.1e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 7.34e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.00107  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 5.58e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00257 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 2.08e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00375  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.03     |\n",
      "|    value_loss         | 5.08e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00122 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 4.41e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00687  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.33     |\n",
      "|    value_loss         | 1.41e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1371      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.33     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.000534 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.49      |\n",
      "|    value_loss         | 5.31e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000315 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.67     |\n",
      "|    value_loss         | 3.2e-08  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1372      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.73e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.86      |\n",
      "|    value_loss         | 3.45e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1372      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.000818 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 7.86e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1372      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -4.08e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.28      |\n",
      "|    value_loss         | 3.64e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000108 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 2.54e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 8.03e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.76     |\n",
      "|    value_loss         | 1.13e-11 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1369        |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 3.66e-05    |\n",
      "|    reward             | -0.00327412 |\n",
      "|    std                | 4.03        |\n",
      "|    value_loss         | 1.91e-10    |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-11-07T22:10:00.000000000 to  2022-11-08T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.37496229712938905\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_8064_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1872         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0017586513 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1738         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015465631 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0184      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000778    |\n",
      "|    reward               | -0.00106505  |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.00067      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1699          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092959433 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.013        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -4.45e-05     |\n",
      "|    reward               | -0.0011878158 |\n",
      "|    std                  | 0.971         |\n",
      "|    value_loss           | 0.000104      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1682        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00426798  |\n",
      "|    clip_fraction        | 0.0253      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    reward               | -0.00054951 |\n",
      "|    std                  | 0.956       |\n",
      "|    value_loss           | 0.000197    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1672          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005437077   |\n",
      "|    clip_fraction        | 0.0555        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0298       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00493      |\n",
      "|    reward               | -0.0008448192 |\n",
      "|    std                  | 0.936         |\n",
      "|    value_loss           | 4.28e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-07T22:10:00.000000000 to  2022-11-08T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.6252961889356883\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_8064_1\n",
      "======DDPG Validation from:  2022-11-07T22:10:00.000000000 to  2022-11-08T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-08T22:10:00.000000000\n",
      "======Trading from:  2022-11-08T22:10:00.000000000 to  2022-11-09T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-08T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_8352_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1311       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0136     |\n",
      "|    reward             | 0.01982368 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.000705   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1337       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.0163     |\n",
      "|    reward             | -0.0145591 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.000371   |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1349          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 0.00125       |\n",
      "|    reward             | -0.0001887168 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 4.6e-07       |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1355     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00209 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 2.33e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1357     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000656 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 5.44e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1360     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00379  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 6.56e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1362      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000519 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 1.49e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1363      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.00947  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 4.6e-05   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1364     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000686 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 8.63e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.00018   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 1.25e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1366      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -9.84e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 3.55e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1366     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 5.73e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 7.78e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1366      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.01     |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 1.55e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1367     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00139  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 4.16e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.00112  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.34     |\n",
      "|    value_loss         | 2.99e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000656 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 1.43e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 8.28e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.68     |\n",
      "|    value_loss         | 1.69e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1369      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000452 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.87      |\n",
      "|    value_loss         | 3.77e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00197 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 6.15e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1369      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.25e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.3       |\n",
      "|    value_loss         | 3.32e-13  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-08T22:10:00.000000000 to  2022-11-09T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.34717488148447395\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_8352_1\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1858          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0005306792 |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002485374 |\n",
      "|    clip_fraction        | 0.027       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00198    |\n",
      "|    reward               | 0.028769843 |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.000708    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1686         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024809362 |\n",
      "|    clip_fraction        | 0.0289       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.85        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00455      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00776     |\n",
      "|    reward               | 0.0003665228 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00829      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1668        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007814305 |\n",
      "|    clip_fraction        | 0.0899      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.55       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00967    |\n",
      "|    reward               | 0.016436527 |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.00922     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1659        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008138107 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.13       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -0.0035382  |\n",
      "|    std                  | 0.987       |\n",
      "|    value_loss           | 0.00463     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-08T22:10:00.000000000 to  2022-11-09T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.35680513997951324\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_8352_1\n",
      "======DDPG Validation from:  2022-11-08T22:10:00.000000000 to  2022-11-09T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-09T22:10:00.000000000\n",
      "======Trading from:  2022-11-09T22:10:00.000000000 to  2022-11-10T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-09T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_8640_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1304        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.00141     |\n",
      "|    reward             | -0.00027452 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 7.15e-06    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1334     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00581  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 1.1e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 8.63e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 5.62e-13 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1352          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.68         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | 6.08e-10      |\n",
      "|    reward             | -0.0004267328 |\n",
      "|    std                | 1.3           |\n",
      "|    value_loss         | 2.05e-19      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1358     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00561  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.4      |\n",
      "|    value_loss         | 8.27e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1358      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000986 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 4.68e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1362     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00336 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.6      |\n",
      "|    value_loss         | 2.82e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1362     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.96    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00385  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 4.8e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1364      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.02     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -9.01e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.83      |\n",
      "|    value_loss         | 6.24e-16  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1365      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -6.91e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.97      |\n",
      "|    value_loss         | 1.69e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1367      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000337 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 2.19e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1367      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -3.02e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.24      |\n",
      "|    value_loss         | 2.55e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00257  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.41     |\n",
      "|    value_loss         | 2.05e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1368     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000117 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.58     |\n",
      "|    value_loss         | 1.49e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1369      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.92e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.76      |\n",
      "|    value_loss         | 3.1e-13   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1369     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.18e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.96     |\n",
      "|    value_loss         | 1.65e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1370     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 7.34e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.17     |\n",
      "|    value_loss         | 6.64e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1370      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.64     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.00227  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.39      |\n",
      "|    value_loss         | 9.82e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1371          |\n",
      "|    iterations         | 1900          |\n",
      "|    time_elapsed       | 6             |\n",
      "|    total_timesteps    | 9500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.71         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1899          |\n",
      "|    policy_loss        | 1.22e-07      |\n",
      "|    reward             | -0.0007373219 |\n",
      "|    std                | 3.63          |\n",
      "|    value_loss         | 5.04e-15      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1371     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.00159 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.88     |\n",
      "|    value_loss         | 3.67e-07 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-09T22:10:00.000000000 to  2022-11-10T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3230786826100729\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_8640_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1864         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0018917798 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1733         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043071806 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    reward               | 0.00033419   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.000533     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1695          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005648528   |\n",
      "|    clip_fraction        | 0.0602        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0262       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00448      |\n",
      "|    reward               | -0.0004000104 |\n",
      "|    std                  | 0.983         |\n",
      "|    value_loss           | 9.27e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1676          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0127413655  |\n",
      "|    clip_fraction        | 0.166         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0456       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0202       |\n",
      "|    reward               | -0.0001965178 |\n",
      "|    std                  | 0.949         |\n",
      "|    value_loss           | 2.45e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1665          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014709039   |\n",
      "|    clip_fraction        | 0.159         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0298       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0243       |\n",
      "|    reward               | -0.0003037616 |\n",
      "|    std                  | 0.913         |\n",
      "|    value_loss           | 1.22e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-09T22:10:00.000000000 to  2022-11-10T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2148574080970078\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_8640_1\n",
      "======DDPG Validation from:  2022-11-09T22:10:00.000000000 to  2022-11-10T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-10T22:10:00.000000000\n",
      "======Trading from:  2022-11-10T22:10:00.000000000 to  2022-11-13T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-10T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_8928_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1301        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.0222      |\n",
      "|    reward             | 0.018697038 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.000646    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000137 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 2e-07    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.000749 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 5.27e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1339      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000146 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 4.74e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1345      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.62     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000841 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.22      |\n",
      "|    value_loss         | 4.42e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1304          |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.68         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | 6.51e-05      |\n",
      "|    reward             | -0.0002084014 |\n",
      "|    std                | 1.3           |\n",
      "|    value_loss         | 2.66e-09      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1254     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00129 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 8.71e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1221     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 3.19e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 3.02e-12 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1221      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.00109  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.6       |\n",
      "|    value_loss         | 3.33e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1219      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.000836 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 1.98e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1224     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00145  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.83     |\n",
      "|    value_loss         | 7.97e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1230      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -3.25e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.96      |\n",
      "|    value_loss         | 3.54e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1234     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00126  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.1      |\n",
      "|    value_loss         | 2.51e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1238      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000657 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.25      |\n",
      "|    value_loss         | 6.19e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1243      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.6e-07   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.4       |\n",
      "|    value_loss         | 7.64e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1251     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.00263 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.58     |\n",
      "|    value_loss         | 1.97e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1254     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00112  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.76     |\n",
      "|    value_loss         | 2.28e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1260      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.37e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.96      |\n",
      "|    value_loss         | 3.35e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1265     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00261  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.17     |\n",
      "|    value_loss         | 8.32e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1270     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.000689 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.4      |\n",
      "|    value_loss         | 1.08e-07 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-10T22:10:00.000000000 to  2022-11-13T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.37228152233192696\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_8928_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1854         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0028427823 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1723         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059953756 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -3.25        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0235      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00405     |\n",
      "|    reward               | 0.0002695115 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0104       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1648          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059275946 |\n",
      "|    clip_fraction        | 0.000391      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0155       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | 8.87e-05      |\n",
      "|    reward               | -0.0010783132 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.000782      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1579          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009012179   |\n",
      "|    clip_fraction        | 0.0828        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00941      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00486      |\n",
      "|    reward               | -0.0004190728 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00023       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1584        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009391064 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | -0.00167908 |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.000233    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-10T22:10:00.000000000 to  2022-11-13T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3475476691463786\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_8928_1\n",
      "======DDPG Validation from:  2022-11-10T22:10:00.000000000 to  2022-11-13T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-13T22:10:00.000000000\n",
      "======Trading from:  2022-11-13T22:10:00.000000000 to  2022-11-14T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-13T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_9216_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1297      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -3.38e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 8.29e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00123 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 1.17e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00269  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 3.15e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00235  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.42     |\n",
      "|    value_loss         | 3.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00464  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 6.46e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00453  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 7.79e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1346     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00219  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.74     |\n",
      "|    value_loss         | 1.06e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1348     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000152 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.86     |\n",
      "|    value_loss         | 9.53e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1349      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -9.94e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.99      |\n",
      "|    value_loss         | 3.49e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 1.75e-09 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 9.03e-19 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1352     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.24    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00521  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.28     |\n",
      "|    value_loss         | 8.5e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1353     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 9.91e-10 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.44     |\n",
      "|    value_loss         | 1.94e-19 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1355      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.38     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -8.34e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.61      |\n",
      "|    value_loss         | 1.56e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1355      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.45     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000702 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.8       |\n",
      "|    value_loss         | 8.79e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1354      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -3.42e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.99      |\n",
      "|    value_loss         | 2.65e-18  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1355      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.58     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.01     |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.2       |\n",
      "|    value_loss         | 1.55e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1355     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.000466 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.42     |\n",
      "|    value_loss         | 3.55e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1356     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00153  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.66     |\n",
      "|    value_loss         | 3.3e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1356     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000339 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.92     |\n",
      "|    value_loss         | 1.64e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1357     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00363  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.2      |\n",
      "|    value_loss         | 2.63e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-13T22:10:00.000000000 to  2022-11-14T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.281487809334102\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_9216_1\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1848          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0001296794 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1722         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004176143  |\n",
      "|    clip_fraction        | 0.00796      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0151      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000262    |\n",
      "|    reward               | 0.0012220445 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.000467     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1685        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009128257 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.972       |\n",
      "|    value_loss           | 0.000113    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1665        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007847195 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0391     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | -0.00153551 |\n",
      "|    std                  | 0.95        |\n",
      "|    value_loss           | 6.67e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1647         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0125336535 |\n",
      "|    clip_fraction        | 0.15         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0102      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0187      |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.918        |\n",
      "|    value_loss           | 3.69e-05     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-11-13T22:10:00.000000000 to  2022-11-14T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0962299410233636\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_9216_1\n",
      "======DDPG Validation from:  2022-11-13T22:10:00.000000000 to  2022-11-14T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-14T22:10:00.000000000\n",
      "======Trading from:  2022-11-14T22:10:00.000000000 to  2022-11-15T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-14T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_9504_1\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1260          |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 0             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.51         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -0.000648     |\n",
      "|    reward             | -0.0005663232 |\n",
      "|    std                | 1.1           |\n",
      "|    value_loss         | 3.81e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1293     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000672 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 3.03e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1308      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000268 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 5.11e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000801 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.36      |\n",
      "|    value_loss         | 3.99e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00215  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 1.69e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.9e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 1.5e-10  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00301  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 4.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00267 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 3.6e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.000934  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 3.29e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00432  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.04     |\n",
      "|    value_loss         | 7.94e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00182 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 5.66e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00412 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.33     |\n",
      "|    value_loss         | 4.16e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00387 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 3.81e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00348 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.67     |\n",
      "|    value_loss         | 3.34e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 1.35e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.86     |\n",
      "|    value_loss         | 4.28e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00119  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 2.97e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1326      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000705 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.29      |\n",
      "|    value_loss         | 7.94e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000186 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.52     |\n",
      "|    value_loss         | 6.96e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00056  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.76     |\n",
      "|    value_loss         | 6.34e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00424  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.03     |\n",
      "|    value_loss         | 2.31e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-14T22:10:00.000000000 to  2022-11-15T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3471771669951604\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_9504_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1811        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00123892 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1687         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052651255 |\n",
      "|    clip_fraction        | 0.0319       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0288      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    reward               | 0.002214885  |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 0.000237     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1654         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012916534 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0191      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000209    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 0.000361     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1637         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010052525 |\n",
      "|    clip_fraction        | 0.000879     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000263    |\n",
      "|    reward               | -0.00128031  |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 0.000169     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1627          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0033539815  |\n",
      "|    clip_fraction        | 0.0136        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.031        |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0015       |\n",
      "|    reward               | -0.0030052303 |\n",
      "|    std                  | 0.96          |\n",
      "|    value_loss           | 0.000217      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-14T22:10:00.000000000 to  2022-11-15T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.6806016567775501\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_9504_1\n",
      "======DDPG Validation from:  2022-11-14T22:10:00.000000000 to  2022-11-15T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-15T22:10:00.000000000\n",
      "======Trading from:  2022-11-15T22:10:00.000000000 to  2022-11-16T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-15T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_9792_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1270        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | -0.257      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.0887      |\n",
      "|    reward             | 0.050417647 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.0074      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1295       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -3.48      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.153      |\n",
      "|    reward             | -0.0506263 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.0169     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1309         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.47        |\n",
      "|    explained_variance | -10.5        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | -0.116       |\n",
      "|    reward             | -0.008133233 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.00847      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1312        |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.49       |\n",
      "|    explained_variance | -355        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.0164     |\n",
      "|    reward             | -0.00518024 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.00156     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1315        |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.5        |\n",
      "|    explained_variance | -10.3       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 0.0459      |\n",
      "|    reward             | -0.00832465 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.00398     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1317        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.51       |\n",
      "|    explained_variance | -0.378      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.0322     |\n",
      "|    reward             | -0.04855817 |\n",
      "|    std                | 1.1         |\n",
      "|    value_loss         | 0.000832    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1319       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.51      |\n",
      "|    explained_variance | -0.116     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.00231   |\n",
      "|    reward             | -0.0057408 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.000231   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1320         |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.52        |\n",
      "|    explained_variance | -0.928       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | -0.0601      |\n",
      "|    reward             | 0.0075439066 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 0.0016       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1320        |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | 0.699       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 0.129       |\n",
      "|    reward             | -0.02858293 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 0.00859     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1319         |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54        |\n",
      "|    explained_variance | -0.1         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | -0.0316      |\n",
      "|    reward             | -0.015496967 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 0.000251     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1316       |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0.143      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -0.0452    |\n",
      "|    reward             | 0.03513851 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.00094    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1317       |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -0.00126   |\n",
      "|    reward             | -0.0356655 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 0.000971   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1318         |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 4            |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.6         |\n",
      "|    explained_variance | -109         |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 0.235        |\n",
      "|    reward             | 0.0055034785 |\n",
      "|    std                | 1.19         |\n",
      "|    value_loss         | 0.0425       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1319       |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.61      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.046     |\n",
      "|    reward             | -0.0084182 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 0.000813   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 0.00042  |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1321        |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.64       |\n",
      "|    explained_variance | 0.377       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -0.0264     |\n",
      "|    reward             | 0.025046753 |\n",
      "|    std                | 1.24        |\n",
      "|    value_loss         | 0.000364    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1322       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.0431     |\n",
      "|    reward             | -0.0246164 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 0.00071    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1322      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.0269   |\n",
      "|    reward             | 0.0018009 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 0.000341  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1323       |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.67      |\n",
      "|    explained_variance | -1.11      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.0208    |\n",
      "|    reward             | -0.0378189 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.000609   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1323       |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.0662    |\n",
      "|    reward             | -0.1026513 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 0.00221    |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-11-15T22:10:00.000000000 to  2022-11-16T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  0.06366392135674165\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_9792_1\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1821          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0001919382 |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1694        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002043839 |\n",
      "|    clip_fraction        | 0.0232      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    reward               | -8.2521e-05 |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.000422    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1657          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009326377   |\n",
      "|    clip_fraction        | 0.084         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0194       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00752      |\n",
      "|    reward               | -0.0001695334 |\n",
      "|    std                  | 0.965         |\n",
      "|    value_loss           | 0.000133      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1628          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00926202    |\n",
      "|    clip_fraction        | 0.1           |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0215       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.011        |\n",
      "|    reward               | -0.0003200775 |\n",
      "|    std                  | 0.94          |\n",
      "|    value_loss           | 0.000111      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1621         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.013132291  |\n",
      "|    clip_fraction        | 0.16         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0211      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0203      |\n",
      "|    reward               | -4.56448e-05 |\n",
      "|    std                  | 0.906        |\n",
      "|    value_loss           | 6.04e-05     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-11-15T22:10:00.000000000 to  2022-11-16T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3415421020154135\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_9792_1\n",
      "======DDPG Validation from:  2022-11-15T22:10:00.000000000 to  2022-11-16T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-16T22:10:00.000000000\n",
      "======Trading from:  2022-11-16T22:10:00.000000000 to  2022-11-17T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-16T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_10080_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1272        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.0166      |\n",
      "|    reward             | 0.036743626 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.00171     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1312         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.47        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.00214      |\n",
      "|    reward             | -0.002382655 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 2e-06        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1320         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | 0.000143     |\n",
      "|    reward             | -0.000574624 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 1.18e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00359  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 4.57e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000467 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 1.24e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1336      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000694 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.49e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1340      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.000835  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.38      |\n",
      "|    value_loss         | 2.55e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1342          |\n",
      "|    iterations         | 800           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 4000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.81         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 799           |\n",
      "|    policy_loss        | 0.00584       |\n",
      "|    reward             | -0.0004352096 |\n",
      "|    std                | 1.48          |\n",
      "|    value_loss         | 1.17e-05      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1343      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.000278  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.58      |\n",
      "|    value_loss         | 3.34e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1345      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.00193  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 1.24e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1347     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000207 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.81     |\n",
      "|    value_loss         | 8.23e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1349      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.00537   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.94      |\n",
      "|    value_loss         | 3.38e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1349     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000506 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.08     |\n",
      "|    value_loss         | 8.6e-08  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1350     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00262  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.22     |\n",
      "|    value_loss         | 1.6e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1351      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000402 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.39      |\n",
      "|    value_loss         | 4.51e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1352      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -4.57e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.56      |\n",
      "|    value_loss         | 4.9e-12   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1353     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0002  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.74     |\n",
      "|    value_loss         | 1.09e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1353     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0011  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.93     |\n",
      "|    value_loss         | 2.43e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1354     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00283  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.14     |\n",
      "|    value_loss         | 1.2e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1355      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000197 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.36      |\n",
      "|    value_loss         | 7.3e-09   |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-16T22:10:00.000000000 to  2022-11-17T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2417723956360187\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_10080_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1831        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.004635226 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1708         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056061684 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.147       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0268      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    reward               | 0.02693335   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.000917     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1666         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029974603 |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -2.55        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0135      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    reward               | 5.78304e-05  |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.0079       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1649         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028002253 |\n",
      "|    clip_fraction        | 0.0107       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.104       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0174      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    reward               | 0.01645969   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00543      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1638          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0018489016  |\n",
      "|    clip_fraction        | 0.00308       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.0911       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0149       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000504     |\n",
      "|    reward               | -0.0034458272 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.00289       |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-16T22:10:00.000000000 to  2022-11-17T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.29273747774395437\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_10080_1\n",
      "======DDPG Validation from:  2022-11-16T22:10:00.000000000 to  2022-11-17T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-17T22:10:00.000000000\n",
      "======Trading from:  2022-11-17T22:10:00.000000000 to  2022-11-20T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-17T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_10368_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1284      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -4.18e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 7.9e-10   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1306      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -4.85e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 1.68e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 0.00094   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 5.79e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00115 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 4.52e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00143 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.44     |\n",
      "|    value_loss         | 5.35e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00142 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.54     |\n",
      "|    value_loss         | 4.72e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00136  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 1.01e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000939 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 3.57e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1324      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.00269  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 1.8e-06   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1326      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.000201 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.03      |\n",
      "|    value_loss         | 1.26e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00151  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 6.79e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000319 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.33      |\n",
      "|    value_loss         | 2.83e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1328      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.33     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -9.86e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.5       |\n",
      "|    value_loss         | 2.21e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.59e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.67      |\n",
      "|    value_loss         | 5.43e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1328      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -4.07e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.86      |\n",
      "|    value_loss         | 3.44e-18  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1329      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -3.73e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 3.01e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00539  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.28     |\n",
      "|    value_loss         | 5.15e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00426  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.52     |\n",
      "|    value_loss         | 3.64e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000251 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.76     |\n",
      "|    value_loss         | 1.07e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.55e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.03     |\n",
      "|    value_loss         | 4.57e-11 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-17T22:10:00.000000000 to  2022-11-20T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2219901111561725\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_10368_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1806         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0046364153 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1680         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008851046  |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.429        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00734     |\n",
      "|    reward               | -0.000587119 |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.00363      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1647          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008702703   |\n",
      "|    clip_fraction        | 0.0691        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.000112     |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00384      |\n",
      "|    reward               | -0.0010675542 |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 0.000183      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1627         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015317678  |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0261      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0216      |\n",
      "|    reward               | -6.40155e-05 |\n",
      "|    std                  | 0.937        |\n",
      "|    value_loss           | 1.73e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1620        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012798401 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.022      |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.898       |\n",
      "|    value_loss           | 7.84e-06    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-17T22:10:00.000000000 to  2022-11-20T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.132661681439701\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_10368_1\n",
      "======DDPG Validation from:  2022-11-17T22:10:00.000000000 to  2022-11-20T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-20T22:10:00.000000000\n",
      "======Trading from:  2022-11-20T22:10:00.000000000 to  2022-11-21T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-20T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_10656_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1279     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000234 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 4.57e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000863 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 5.21e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.000164 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 2.79e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00106  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.38     |\n",
      "|    value_loss         | 5.05e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00407  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 6.39e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1331      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.43e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 9.72e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1333      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.00141   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 8.06e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1335      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -8.41e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.8       |\n",
      "|    value_loss         | 2.67e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1337     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00153 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.93     |\n",
      "|    value_loss         | 7.43e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1337      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -9.65e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.07      |\n",
      "|    value_loss         | 2.96e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1337     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -4.6e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.22     |\n",
      "|    value_loss         | 6.75e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1336     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.000442 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.37     |\n",
      "|    value_loss         | 4.2e-08  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1337     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 3.52e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.54     |\n",
      "|    value_loss         | 3.65e-14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1337     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00524  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.73     |\n",
      "|    value_loss         | 3.43e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1338     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000282 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.92     |\n",
      "|    value_loss         | 1.52e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1339     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.000863 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.13     |\n",
      "|    value_loss         | 1.2e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1339      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.0025    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.36      |\n",
      "|    value_loss         | 1.29e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1340      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.74e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.6       |\n",
      "|    value_loss         | 1.01e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1340     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000969 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.85     |\n",
      "|    value_loss         | 1.37e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1341     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -3e-05   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.12     |\n",
      "|    value_loss         | 1.34e-10 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-20T22:10:00.000000000 to  2022-11-21T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.5378896232175547\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_10656_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1822       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00097598 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1681         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028281498 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000524    |\n",
      "|    reward               | 0.001323556  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000596     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1646         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007373823 |\n",
      "|    clip_fraction        | 0.000391     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0202      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    reward               | -0.00149838  |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 0.000204     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1631          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007115688   |\n",
      "|    clip_fraction        | 0.0895        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.035        |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00918      |\n",
      "|    reward               | -0.0004606333 |\n",
      "|    std                  | 0.967         |\n",
      "|    value_loss           | 8.74e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1619          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010625588   |\n",
      "|    clip_fraction        | 0.111         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0327       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0131       |\n",
      "|    reward               | -0.0013686736 |\n",
      "|    std                  | 0.94          |\n",
      "|    value_loss           | 8.05e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-20T22:10:00.000000000 to  2022-11-21T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -0.9748404424816863\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_10656_1\n",
      "======DDPG Validation from:  2022-11-20T22:10:00.000000000 to  2022-11-21T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-21T22:10:00.000000000\n",
      "======Trading from:  2022-11-21T22:10:00.000000000 to  2022-11-22T22:10:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-21T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_10944_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1228     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00181 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.07     |\n",
      "|    value_loss         | 1.83e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1277     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000983 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 3.82e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1295      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000696 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 2.89e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.000444 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 1.24e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00139  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.43     |\n",
      "|    value_loss         | 8.91e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.000563 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.53     |\n",
      "|    value_loss         | 1.39e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1321      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000496 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 1.14e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1323      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.02e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.75      |\n",
      "|    value_loss         | 4.99e-13  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1325          |\n",
      "|    iterations         | 900           |\n",
      "|    time_elapsed       | 3             |\n",
      "|    total_timesteps    | 4500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.05         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 899           |\n",
      "|    policy_loss        | 3.79e-06      |\n",
      "|    reward             | -0.0003986918 |\n",
      "|    std                | 1.88          |\n",
      "|    value_loss         | 3.36e-12      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -5.08e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.01      |\n",
      "|    value_loss         | 8.18e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1329      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000935 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.15      |\n",
      "|    value_loss         | 2.3e-07   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1331      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.25     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000872 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.3       |\n",
      "|    value_loss         | 2.61e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1332     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.32    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00359 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.47     |\n",
      "|    value_loss         | 2.27e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00367 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.64     |\n",
      "|    value_loss         | 3.26e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1332      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.46     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -3.98e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.83      |\n",
      "|    value_loss         | 3.85e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00448  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.03     |\n",
      "|    value_loss         | 2.72e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1334      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 4.45e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.25      |\n",
      "|    value_loss         | 4.55e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1335      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.66     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -3.14e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.48      |\n",
      "|    value_loss         | 2.89e-14  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1335          |\n",
      "|    iterations         | 1900          |\n",
      "|    time_elapsed       | 7             |\n",
      "|    total_timesteps    | 9500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.73         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1899          |\n",
      "|    policy_loss        | -0.00222      |\n",
      "|    reward             | -0.0009726374 |\n",
      "|    std                | 3.71          |\n",
      "|    value_loss         | 6.54e-07      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1336      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.25e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.98      |\n",
      "|    value_loss         | 1.44e-15  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-21T22:10:00.000000000 to  2022-11-22T22:10:00.000000000\n",
      "A2C Sharpe Ratio:  -0.20535443303647657\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_10944_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1797         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0022135873 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1667        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007573696 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00751    |\n",
      "|    reward               | 0.000332089 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00785     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1631          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006592635   |\n",
      "|    clip_fraction        | 0.048         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0208       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00366      |\n",
      "|    reward               | -0.0009747234 |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 0.00163       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1616        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009720148 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 0.000969    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1609          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0101860855  |\n",
      "|    clip_fraction        | 0.177         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0186       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.012        |\n",
      "|    reward               | -0.0003326512 |\n",
      "|    std                  | 0.955         |\n",
      "|    value_loss           | 0.000848      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-21T22:10:00.000000000 to  2022-11-22T22:10:00.000000000\n",
      "PPO Sharpe Ratio:  -1.341948990897319\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_10944_1\n",
      "======DDPG Validation from:  2022-11-21T22:10:00.000000000 to  2022-11-22T22:10:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-22T22:10:00.000000000\n",
      "======Trading from:  2022-11-22T22:10:00.000000000 to  2022-11-23T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-22T22:10:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_11232_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1277       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0424     |\n",
      "|    reward             | 0.03460708 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.00172    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1307         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.44        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.0224       |\n",
      "|    reward             | -0.019129083 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.000501     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1315         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.46        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | 0.000631     |\n",
      "|    reward             | -0.001329188 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 1.77e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1319         |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.5         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -0.00757     |\n",
      "|    reward             | -0.002975135 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 4.14e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1322         |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | -14.8        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 0.0113       |\n",
      "|    reward             | -0.007145583 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 0.00356      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1324        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.52       |\n",
      "|    explained_variance | -12.7       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.0162     |\n",
      "|    reward             | -0.04711827 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.00201     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1326         |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 0.002        |\n",
      "|    reward             | -0.000374328 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 2.59e-06     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1327          |\n",
      "|    iterations         | 800           |\n",
      "|    time_elapsed       | 3             |\n",
      "|    total_timesteps    | 4000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.55         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 799           |\n",
      "|    policy_loss        | 0.000396      |\n",
      "|    reward             | -0.0003260664 |\n",
      "|    std                | 1.15          |\n",
      "|    value_loss         | 3.88e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000883 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 4.67e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00117 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 9.21e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1332      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000207 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 6.95e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 5.61e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 3.18e-16 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1330      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.28e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.46      |\n",
      "|    value_loss         | 1.43e-14  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1329          |\n",
      "|    iterations         | 1400          |\n",
      "|    time_elapsed       | 5             |\n",
      "|    total_timesteps    | 7000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.87         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1399          |\n",
      "|    policy_loss        | 0.00062       |\n",
      "|    reward             | -0.0001532592 |\n",
      "|    std                | 1.57          |\n",
      "|    value_loss         | 1.51e-07      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1330      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.000449 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 7.31e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1330      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.000604 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.8       |\n",
      "|    value_loss         | 1.16e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1331      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.00318   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 2.35e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1e-06    |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.06     |\n",
      "|    value_loss         | 1.92e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1331      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.000231 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 1.53e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1332      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000207 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 9.61e-09  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-22T22:10:00.000000000 to  2022-11-23T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.16687669891237533\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_11232_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1809        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.004230872 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1668         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016977156 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0194      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    reward               | 0.023001859  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00267      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1633         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047818944 |\n",
      "|    clip_fraction        | 0.0365       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -13.2        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    reward               | 0.000851164  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.018        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1616         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033711768 |\n",
      "|    clip_fraction        | 0.0169       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.296       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0271      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00244     |\n",
      "|    reward               | 0.01487269   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00668      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1609         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072599906 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.273       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00849     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00542     |\n",
      "|    reward               | -0.00505888  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0046       |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-11-22T22:10:00.000000000 to  2022-11-23T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -0.1773562490556633\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_11232_1\n",
      "======DDPG Validation from:  2022-11-22T22:10:00.000000000 to  2022-11-23T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-23T22:20:00.000000000\n",
      "======Trading from:  2022-11-23T22:20:00.000000000 to  2022-11-24T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-23T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_11520_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1275        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.52       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.000309    |\n",
      "|    reward             | -0.00135812 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 6.31e-08    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000401 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.25e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00125  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 1.29e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00372 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 9.38e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00481 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 8.99e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.000916 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 3.39e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00221 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 1.65e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2       |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00766 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 9.91e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000731 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.92      |\n",
      "|    value_loss         | 1.35e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1328      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.0028    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.06      |\n",
      "|    value_loss         | 1.83e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1329     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000571 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 1.08e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00675  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.37     |\n",
      "|    value_loss         | 9.67e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 1.38e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.53     |\n",
      "|    value_loss         | 3.69e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1330     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00361 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.71     |\n",
      "|    value_loss         | 3.81e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00341 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.91     |\n",
      "|    value_loss         | 3.36e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1331     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00408  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.11     |\n",
      "|    value_loss         | 4.1e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1332      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -4.64e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.32      |\n",
      "|    value_loss         | 3.52e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 1.97e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.57     |\n",
      "|    value_loss         | 8.98e-13 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1333     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000217 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.82     |\n",
      "|    value_loss         | 8.71e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1334      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -7.12e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.09      |\n",
      "|    value_loss         | 1.09e-09  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-23T22:20:00.000000000 to  2022-11-24T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3610820187435028\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_11520_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1796         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0014233002 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1674        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004353627 |\n",
      "|    clip_fraction        | 0.0165      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000396   |\n",
      "|    reward               | 0.001458295 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000833    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1633          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003769427   |\n",
      "|    clip_fraction        | 0.0232        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0203       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00261      |\n",
      "|    reward               | -0.0003457232 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 0.000149      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1611         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069065588 |\n",
      "|    clip_fraction        | 0.0858       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0156      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00882     |\n",
      "|    reward               | -0.00128031  |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 0.000147     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1603        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011435203 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    reward               | -0.00117396 |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 6.86e-05    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-23T22:20:00.000000000 to  2022-11-24T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -1.44335487253336\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_11520_1\n",
      "======DDPG Validation from:  2022-11-23T22:20:00.000000000 to  2022-11-24T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-24T22:20:00.000000000\n",
      "======Trading from:  2022-11-24T22:20:00.000000000 to  2022-11-27T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-24T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_11808_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1277     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000912 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 7.5e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1303      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000512 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 5.02e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1311      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.000874 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 3.78e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1316      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000867 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.38      |\n",
      "|    value_loss         | 3.91e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00143  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 9.29e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000466 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.58      |\n",
      "|    value_loss         | 8.07e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0013   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.69     |\n",
      "|    value_loss         | 4.5e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 3.46e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.81     |\n",
      "|    value_loss         | 3.31e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1324      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.000479  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 6.11e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1324      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -3.45e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.07      |\n",
      "|    value_loss         | 3.02e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1325      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -9.58e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.22      |\n",
      "|    value_loss         | 2.85e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1326      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.000775 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.38      |\n",
      "|    value_loss         | 1.73e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00558 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.55     |\n",
      "|    value_loss         | 6.92e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0115   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.72     |\n",
      "|    value_loss         | 2.5e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1327     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.00096  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.92     |\n",
      "|    value_loss         | 2.53e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.08e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.13     |\n",
      "|    value_loss         | 4.15e-15 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1328     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00122 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.35     |\n",
      "|    value_loss         | 3.22e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00629 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.58     |\n",
      "|    value_loss         | 3.5e-06  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.37e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.83      |\n",
      "|    value_loss         | 2.87e-19  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1328      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -2.51e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.09      |\n",
      "|    value_loss         | 9.7e-17   |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-24T22:20:00.000000000 to  2022-11-27T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.24588269830808676\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_11808_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1783         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0019195568 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1667         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031922332 |\n",
      "|    clip_fraction        | 0.0083       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    reward               | -0.000639644 |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.000881     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1623        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005453511 |\n",
      "|    clip_fraction        | 0.0286      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    reward               | -0.00152718 |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 0.00056     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1602          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.01115263    |\n",
      "|    clip_fraction        | 0.145         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0227       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0126       |\n",
      "|    reward               | -0.0008008581 |\n",
      "|    std                  | 0.948         |\n",
      "|    value_loss           | 0.00014       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1590          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012439154   |\n",
      "|    clip_fraction        | 0.167         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.043        |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0214       |\n",
      "|    reward               | -0.0008068848 |\n",
      "|    std                  | 0.918         |\n",
      "|    value_loss           | 0.000149      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-24T22:20:00.000000000 to  2022-11-27T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -1.1910011413251151\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_11808_1\n",
      "======DDPG Validation from:  2022-11-24T22:20:00.000000000 to  2022-11-27T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-27T22:20:00.000000000\n",
      "======Trading from:  2022-11-27T22:20:00.000000000 to  2022-11-28T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-27T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_12096_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1243        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.00625     |\n",
      "|    reward             | -0.00071312 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 4.54e-05    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1284       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.00715    |\n",
      "|    reward             | -0.0092083 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.15e-05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00654  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.6e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 7.56e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 3.87e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1312      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -6.35e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 2.83e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 1.79e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0028   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.44     |\n",
      "|    value_loss         | 4.23e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00424 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 2.6e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00309  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.65     |\n",
      "|    value_loss         | 4.03e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1318      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.00363   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 4.97e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -4.85e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 5.96e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00382 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.04     |\n",
      "|    value_loss         | 1.87e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -7e-05   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 1.16e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000651 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.33     |\n",
      "|    value_loss         | 1.06e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 8.34e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 1.99e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 1.73e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 4.61e-11  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1320        |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 0.000309    |\n",
      "|    reward             | -0.00156274 |\n",
      "|    std                | 2.87        |\n",
      "|    value_loss         | 1.56e-08    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 9.03e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 1.56e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1287      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -1.48e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.29      |\n",
      "|    value_loss         | 2.18e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1290      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.00119   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.53      |\n",
      "|    value_loss         | 1.81e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1293     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.000131 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.78     |\n",
      "|    value_loss         | 3.52e-09 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-27T22:20:00.000000000 to  2022-11-28T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.31944984965570306\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_12096_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1791       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00320978 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1668         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011919991 |\n",
      "|    clip_fraction        | 0.0021       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 0.000167     |\n",
      "|    reward               | 0.017851636  |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.00169      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1631        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008740265 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.531      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00819    |\n",
      "|    reward               | -0.00144978 |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 0.0089      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1613          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006719807   |\n",
      "|    clip_fraction        | 0.0784        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0128       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0053       |\n",
      "|    reward               | -0.0008378759 |\n",
      "|    std                  | 0.973         |\n",
      "|    value_loss           | 0.000139      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1604          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012182474   |\n",
      "|    clip_fraction        | 0.152         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0251       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0177       |\n",
      "|    reward               | -0.0002972512 |\n",
      "|    std                  | 0.939         |\n",
      "|    value_loss           | 3.84e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-27T22:20:00.000000000 to  2022-11-28T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -1.4496094541364666\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_12096_1\n",
      "======DDPG Validation from:  2022-11-27T22:20:00.000000000 to  2022-11-28T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-28T22:20:00.000000000\n",
      "======Trading from:  2022-11-28T22:20:00.000000000 to  2022-11-29T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-28T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_12384_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1272     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 5.4e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 2.48e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1304      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000369 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.21      |\n",
      "|    value_loss         | 7.95e-08  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1309          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.68         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 7.23e-05      |\n",
      "|    reward             | -0.0005384448 |\n",
      "|    std                | 1.3           |\n",
      "|    value_loss         | 3.26e-09      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00107 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 3.95e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000356 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 8.04e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00298  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 3.66e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00989  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 1.25e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00262  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.82     |\n",
      "|    value_loss         | 2.36e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1324      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000105 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.95      |\n",
      "|    value_loss         | 4.86e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1325      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.000673 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 1.28e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00202 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.24     |\n",
      "|    value_loss         | 6.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1324     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00315 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.41     |\n",
      "|    value_loss         | 2.3e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1325     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000765 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.58     |\n",
      "|    value_loss         | 1.7e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000804 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.76     |\n",
      "|    value_loss         | 1.81e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1326      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -2.24e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.95      |\n",
      "|    value_loss         | 1.45e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00014  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.16     |\n",
      "|    value_loss         | 3.6e-09  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1327      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.64     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -7.27e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.38      |\n",
      "|    value_loss         | 9.44e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1325      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -6.76e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.61      |\n",
      "|    value_loss         | 9.93e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1325      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -4.96e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.86      |\n",
      "|    value_loss         | 4.22e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1326     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 4.39e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.14     |\n",
      "|    value_loss         | 3.75e-10 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-28T22:20:00.000000000 to  2022-11-29T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.20215546706827686\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_12384_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1782         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0025129744 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1657        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001476946 |\n",
      "|    clip_fraction        | 0.00273     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.0668     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000582   |\n",
      "|    reward               | 0.021426715 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0019      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1618         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031793825 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00053     |\n",
      "|    reward               | 0.0007716768 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00165      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1601         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035339105 |\n",
      "|    clip_fraction        | 0.0381       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.69        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0169      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00403     |\n",
      "|    reward               | 0.01475669   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00972      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1591          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0027934648  |\n",
      "|    clip_fraction        | 0.00859       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.53         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0278       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00142      |\n",
      "|    reward               | -0.0049589663 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.00412       |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-28T22:20:00.000000000 to  2022-11-29T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -0.31284332836977113\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_12384_1\n",
      "======DDPG Validation from:  2022-11-28T22:20:00.000000000 to  2022-11-29T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-29T22:20:00.000000000\n",
      "======Trading from:  2022-11-29T22:20:00.000000000 to  2022-11-30T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-29T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_12672_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1254        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | 0.16        |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.17        |\n",
      "|    reward             | 0.053342693 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.0099      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1288         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | 0.474        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.0368       |\n",
      "|    reward             | -0.050452463 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 0.00151      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1299       |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | -7.35      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 0.0358     |\n",
      "|    reward             | -0.0069883 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.00221    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1303       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.0261    |\n",
      "|    reward             | -0.0044422 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.000585   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1306       |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -0.0167    |\n",
      "|    reward             | -0.0069773 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.000557   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1308       |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -1.26      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.0379    |\n",
      "|    reward             | -0.0488257 |\n",
      "|    std                | 1.09       |\n",
      "|    value_loss         | 0.000596   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1311         |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | -0.0376      |\n",
      "|    reward             | -0.004562076 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 0.00132      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1311      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.0123   |\n",
      "|    reward             | 0.0082329 |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 0.000304  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1312       |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 0.098      |\n",
      "|    reward             | -0.0284895 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.00598    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1311       |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -0.00806   |\n",
      "|    reward             | -0.0158275 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 9.54e-05   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0255  |\n",
      "|    reward             | 0.037986 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 0.000336 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1312      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.0431    |\n",
      "|    reward             | -0.037347 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.000891  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1313      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.069     |\n",
      "|    reward             | 0.0075936 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 0.00317   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1314       |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.6       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.0431    |\n",
      "|    reward             | -0.0088578 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 0.000861   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 0.000418 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    reward             | 0.026565 |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 0.000309 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1314       |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.63      |\n",
      "|    explained_variance | -6.09      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.0175     |\n",
      "|    reward             | -0.0275026 |\n",
      "|    std                | 1.24       |\n",
      "|    value_loss         | 0.000825   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.0297   |\n",
      "|    reward             | 0.0018951 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 0.000491  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1315        |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -0.0241     |\n",
      "|    reward             | -0.04044448 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 0.000566    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | -9.86    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.176    |\n",
      "|    reward             | -0.10773 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 0.00606  |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-11-29T22:20:00.000000000 to  2022-11-30T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.08608830002762057\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_12672_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1778         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0016851051 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1655        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010427628 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.607       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    reward               | 0.028690198 |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 0.0183      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1610         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010483533  |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.577       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00658     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00991     |\n",
      "|    reward               | 0.0005273742 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0191       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1596          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010180768   |\n",
      "|    clip_fraction        | 0.139         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.605         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0112       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0121       |\n",
      "|    reward               | -0.0009786201 |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.00928       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1588        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008228053 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0297      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    reward               | -0.00164548 |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 0.00168     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-11-29T22:20:00.000000000 to  2022-11-30T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -1.4873659463805484\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_12672_1\n",
      "======DDPG Validation from:  2022-11-29T22:20:00.000000000 to  2022-11-30T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-11-30T22:20:00.000000000\n",
      "======Trading from:  2022-11-30T22:20:00.000000000 to  2022-12-01T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-11-30T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_12960_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1261        |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 0.00115     |\n",
      "|    reward             | -0.00071312 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 1.51e-06    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1292      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000638 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 1.76e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1300      |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.0057   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 7.35e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1304      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000891 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 4.27e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000493 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.38     |\n",
      "|    value_loss         | 1.52e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00188 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 2e-06    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -8e-06   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 1.45e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00524  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.7      |\n",
      "|    value_loss         | 9.08e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00292  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.82     |\n",
      "|    value_loss         | 2.69e-06 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1315        |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -0.000627   |\n",
      "|    reward             | -0.00178871 |\n",
      "|    std                | 1.95        |\n",
      "|    value_loss         | 1.03e-07    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00291  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 3.1e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00107  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.24     |\n",
      "|    value_loss         | 2.3e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1316      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.00154  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.39      |\n",
      "|    value_loss         | 5.7e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000623 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.57     |\n",
      "|    value_loss         | 1.03e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1318      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.00329  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.75      |\n",
      "|    value_loss         | 2.54e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 4.74e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.95      |\n",
      "|    value_loss         | 5.27e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.000192 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.15     |\n",
      "|    value_loss         | 5.71e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000642 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.38     |\n",
      "|    value_loss         | 7.62e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 1.28e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.63     |\n",
      "|    value_loss         | 3.25e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1321      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 0.00662   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.88      |\n",
      "|    value_loss         | 9.08e-06  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-11-30T22:20:00.000000000 to  2022-12-01T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.3071711783618558\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_12960_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1770         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0027907647 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1658         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038657729 |\n",
      "|    clip_fraction        | 0.0206       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    reward               | 0.02159575   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00145      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1614        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00194962  |\n",
      "|    clip_fraction        | 0.0194      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -3.23       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    reward               | -0.00050538 |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 0.0173      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1596         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.014215     |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.843       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00741     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00868     |\n",
      "|    reward               | 0.0015153171 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00819      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1588          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0013207404  |\n",
      "|    clip_fraction        | 0.00537       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0146       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000157     |\n",
      "|    reward               | -0.0005391856 |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 0.00127       |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-11-30T22:20:00.000000000 to  2022-12-01T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -0.7185522327345214\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_12960_1\n",
      "======DDPG Validation from:  2022-11-30T22:20:00.000000000 to  2022-12-01T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-01T22:20:00.000000000\n",
      "======Trading from:  2022-12-01T22:20:00.000000000 to  2022-12-04T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-01T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_13248_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1253       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.033      |\n",
      "|    reward             | 0.04286308 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.00246    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000578 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 4.16e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.000123 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 1.14e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00034  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 5.85e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 3.98e-09 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 8.21e-17 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1311      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -4.26e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 4.58e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1313      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000646 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.38      |\n",
      "|    value_loss         | 1.03e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 0.00176   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 1.55e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1315      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.00321  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.58      |\n",
      "|    value_loss         | 5.88e-06  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1316        |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 0.000649    |\n",
      "|    reward             | -0.00153871 |\n",
      "|    std                | 1.69        |\n",
      "|    value_loss         | 1.67e-07    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1317      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000726 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.8       |\n",
      "|    value_loss         | 1.91e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0025   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.93     |\n",
      "|    value_loss         | 1.99e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00326  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 3.77e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00283  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 2.62e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -4.19e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.38      |\n",
      "|    value_loss         | 5.51e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.000435 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.54      |\n",
      "|    value_loss         | 6.29e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 5.9e-06  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.72     |\n",
      "|    value_loss         | 5.27e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00613  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.91     |\n",
      "|    value_loss         | 3.36e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1319      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.55     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 0.000144  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.12      |\n",
      "|    value_loss         | 5.24e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.83e-09 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.33     |\n",
      "|    value_loss         | 7.42e-19 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-01T22:20:00.000000000 to  2022-12-04T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2736146181560924\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_13248_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1769         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0044773133 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1650         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023720907 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.964       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0278      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00617     |\n",
      "|    reward               | 0.029061286  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0203       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1610        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007834829 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -1.12       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 0.001093441 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.0165      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1593         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058325003 |\n",
      "|    clip_fraction        | 0.0597       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.53        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00236      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00915     |\n",
      "|    reward               | 0.017362794  |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 0.00668      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1583         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049342797 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.152       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.02        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    reward               | -0.003552    |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.00454      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-12-01T22:20:00.000000000 to  2022-12-04T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -0.1978780334630146\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_13248_1\n",
      "======DDPG Validation from:  2022-12-01T22:20:00.000000000 to  2022-12-04T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-04T22:20:00.000000000\n",
      "======Trading from:  2022-12-04T22:20:00.000000000 to  2022-12-05T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-04T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_13536_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1260     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00124 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 1.09e-06 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1289         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.6         |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | -0.000484    |\n",
      "|    reward             | -0.000421974 |\n",
      "|    std                | 1.2          |\n",
      "|    value_loss         | 5.58e-08     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00413 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 1.21e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00213  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 1.13e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000627 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 1.6e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00147 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 1.09e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -3.74e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 3.64e-16  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1311      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000256 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.82      |\n",
      "|    value_loss         | 1.36e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1312      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000833 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.95      |\n",
      "|    value_loss         | 1.02e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00034  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 3.39e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1315      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -7.34e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.22      |\n",
      "|    value_loss         | 1.6e-11   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0024  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.38     |\n",
      "|    value_loss         | 1.35e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.35     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -6.34e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.55      |\n",
      "|    value_loss         | 5.46e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.42    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00982 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.73     |\n",
      "|    value_loss         | 1.69e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00291 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.92     |\n",
      "|    value_loss         | 1.13e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.56     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 5.22e-06  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.13      |\n",
      "|    value_loss         | 4.17e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00224  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.35     |\n",
      "|    value_loss         | 5.01e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 3.51e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.58     |\n",
      "|    value_loss         | 2.58e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1316      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.61e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.83      |\n",
      "|    value_loss         | 4.37e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 9.06e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.11     |\n",
      "|    value_loss         | 1.87e-16 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-04T22:20:00.000000000 to  2022-12-05T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.255191548590634\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_13536_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1762       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00095708 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1647         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051174713 |\n",
      "|    clip_fraction        | 0.0273       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0184      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    reward               | 0.02749933   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.00022      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1607         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021234464 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -10.4        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0228      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    reward               | 0.000973041  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0109       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1591         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011942695 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.536       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0158      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000347    |\n",
      "|    reward               | 0.0171535    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00601      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1583        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004922469 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.249      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    reward               | -0.00355768 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00394     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-12-04T22:20:00.000000000 to  2022-12-05T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -0.362719589899044\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_13536_1\n",
      "======DDPG Validation from:  2022-12-04T22:20:00.000000000 to  2022-12-05T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-05T22:20:00.000000000\n",
      "======Trading from:  2022-12-05T22:20:00.000000000 to  2022-12-06T22:20:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-05T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_13824_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1254       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | -3.88      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -0.0893    |\n",
      "|    reward             | 0.05333241 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.0167     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1284       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.023      |\n",
      "|    reward             | -0.0497527 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 0.0017     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1294          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.49         |\n",
      "|    explained_variance | -25.7         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | -0.0992       |\n",
      "|    reward             | -0.0076807938 |\n",
      "|    std                | 1.07          |\n",
      "|    value_loss         | 0.00877       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1299          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.000188     |\n",
      "|    reward             | -0.0012255984 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 1.11e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1303     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.000718 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.1      |\n",
      "|    value_loss         | 2.56e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00144  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 1.81e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000475 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 7.65e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1309      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.55e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 8.6e-17   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000511 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 9.13e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00127 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.41     |\n",
      "|    value_loss         | 6.67e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00387 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 2.99e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.9     |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.00146  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.61     |\n",
      "|    value_loss         | 9.67e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1314     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00284  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.73     |\n",
      "|    value_loss         | 3.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.03    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.000874 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.85     |\n",
      "|    value_loss         | 2.26e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00286 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.98     |\n",
      "|    value_loss         | 1.64e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1315      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -2.04e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 1.71e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.24    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00104 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.27     |\n",
      "|    value_loss         | 3e-07    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1316      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.31     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000917 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.44      |\n",
      "|    value_loss         | 2.69e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1317      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -9.47e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.6       |\n",
      "|    value_loss         | 1.88e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.44    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 1.44e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.78     |\n",
      "|    value_loss         | 4.75e-15 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-05T22:20:00.000000000 to  2022-12-06T22:20:00.000000000\n",
      "A2C Sharpe Ratio:  -0.33194840267815123\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_13824_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1759         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0051228516 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1647         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066227177 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.265       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00402     |\n",
      "|    reward               | 0.0005239435 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00824      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1607          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006220999   |\n",
      "|    clip_fraction        | 0.0358        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.516         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0243       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00251      |\n",
      "|    reward               | -0.0007940054 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 0.00121       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1587         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009899378  |\n",
      "|    clip_fraction        | 0.102        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0215      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0113      |\n",
      "|    reward               | -1.45899e-05 |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 0.000244     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1579       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01340089 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.36      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.00525   |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    reward               | 0.0        |\n",
      "|    std                  | 0.927      |\n",
      "|    value_loss           | 0.000153   |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-12-05T22:20:00.000000000 to  2022-12-06T22:20:00.000000000\n",
      "PPO Sharpe Ratio:  -1.0936291418763975\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_13824_1\n",
      "======DDPG Validation from:  2022-12-05T22:20:00.000000000 to  2022-12-06T22:20:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-06T22:20:00.000000000\n",
      "======Trading from:  2022-12-06T22:20:00.000000000 to  2022-12-07T22:25:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-06T22:20:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_14112_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1246         |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.46        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | 0.000678     |\n",
      "|    reward             | 0.0006410192 |\n",
      "|    std                | 1.05         |\n",
      "|    value_loss         | 4.96e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.000213 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 3.85e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.13e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 1.04e-10 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1299        |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -0.00298    |\n",
      "|    reward             | -0.00157128 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 1.48e-06    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00047 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 6.36e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00149  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.46     |\n",
      "|    value_loss         | 6.66e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00515 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.56     |\n",
      "|    value_loss         | 3.04e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00267 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.67     |\n",
      "|    value_loss         | 1.87e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1241      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -8.08e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 2.91e-11  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1249     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 9.54e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.91     |\n",
      "|    value_loss         | 2.79e-15 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1256     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00408  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.05     |\n",
      "|    value_loss         | 5.19e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1258     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0064  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.19     |\n",
      "|    value_loss         | 5.07e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1263     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 2.01e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.35     |\n",
      "|    value_loss         | 1.32e-12 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1267     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00161 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.51     |\n",
      "|    value_loss         | 4.13e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1271     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00063 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.69     |\n",
      "|    value_loss         | 8.72e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1274     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00895  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.88     |\n",
      "|    value_loss         | 1.87e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1276      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -4.24e-10 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.09      |\n",
      "|    value_loss         | 4.53e-20  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1279      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000621 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.31      |\n",
      "|    value_loss         | 5.92e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1282     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000403 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.55     |\n",
      "|    value_loss         | 3.23e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.00449  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.8      |\n",
      "|    value_loss         | 4.43e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-06T22:20:00.000000000 to  2022-12-07T22:25:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2615763687622183\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_14112_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1762       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.00108938 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1644         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023519907 |\n",
      "|    clip_fraction        | 0.00811      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0176      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000443    |\n",
      "|    reward               | 0.026653785  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 0.000907     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1607         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005411893  |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -4.95        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0172      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00658     |\n",
      "|    reward               | -8.51286e-05 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.0132       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1589        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007550253 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.785      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00716    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00868    |\n",
      "|    reward               | 0.0166858   |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 0.0092      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1573         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044084396 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.493       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00222      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    reward               | -0.00500008  |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.00561      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-12-06T22:20:00.000000000 to  2022-12-07T22:25:00.000000000\n",
      "PPO Sharpe Ratio:  -0.10190383271175257\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_14112_1\n",
      "======DDPG Validation from:  2022-12-06T22:20:00.000000000 to  2022-12-07T22:25:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-07T22:25:00.000000000\n",
      "======Trading from:  2022-12-07T22:25:00.000000000 to  2022-12-08T22:25:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-07T22:25:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_14400_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1248     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.00299  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 4.46e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1279     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00218  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 1.47e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1291     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00125  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 5.89e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1296      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000556 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.39      |\n",
      "|    value_loss         | 1.09e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.00213  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 1.57e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1302     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00437 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 7.78e-06 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1302       |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 0.000194   |\n",
      "|    reward             | -0.0015597 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 1.79e-08   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00145 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.82     |\n",
      "|    value_loss         | 4.86e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000541 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.94     |\n",
      "|    value_loss         | 1.11e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.00221  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 1.31e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 2.66e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.24     |\n",
      "|    value_loss         | 1.52e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 1.64e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.4      |\n",
      "|    value_loss         | 5.67e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000197 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.56     |\n",
      "|    value_loss         | 9.83e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00193 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.75     |\n",
      "|    value_loss         | 1.07e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000531 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.94     |\n",
      "|    value_loss         | 6.59e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1309      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -3.79e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.15      |\n",
      "|    value_loss         | 1.96e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.63    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00475 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.37     |\n",
      "|    value_loss         | 2.85e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.46e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.61      |\n",
      "|    value_loss         | 1.14e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -2.69e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.86      |\n",
      "|    value_loss         | 1.23e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 3.39e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.14     |\n",
      "|    value_loss         | 2.27e-12 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-07T22:25:00.000000000 to  2022-12-08T22:25:00.000000000\n",
      "A2C Sharpe Ratio:  0.0\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_14400_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1746        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00052972 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1639         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014403479 |\n",
      "|    clip_fraction        | 0.00503      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0156      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    reward               | -0.00074825  |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 0.00105      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1592          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0066384566  |\n",
      "|    clip_fraction        | 0.0542        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0239       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00408      |\n",
      "|    reward               | -0.0004193694 |\n",
      "|    std                  | 0.973         |\n",
      "|    value_loss           | 6.56e-05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1578         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074176383 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0238      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00727     |\n",
      "|    reward               | -0.00128611  |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 6.35e-05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1569          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.011471946   |\n",
      "|    clip_fraction        | 0.148         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0256       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0181       |\n",
      "|    reward               | -0.0010045985 |\n",
      "|    std                  | 0.918         |\n",
      "|    value_loss           | 3.34e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-12-07T22:25:00.000000000 to  2022-12-08T22:25:00.000000000\n",
      "PPO Sharpe Ratio:  -1.3086858035527609\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_14400_1\n",
      "======DDPG Validation from:  2022-12-07T22:25:00.000000000 to  2022-12-08T22:25:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-08T22:25:00.000000000\n",
      "======Trading from:  2022-12-08T22:25:00.000000000 to  2022-12-11T22:25:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-08T22:25:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_14688_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1263     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00345 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 2.29e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.00123 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 5.95e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00125 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 5.59e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00271 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 5.32e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1303      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -5.69e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 1.61e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1305      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -8.44e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 1.86e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000475 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 9.02e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00266 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 1.94e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00328  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 3.36e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00112 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.04     |\n",
      "|    value_loss         | 4.17e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1308      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000897 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 2.15e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00507 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.33     |\n",
      "|    value_loss         | 5.25e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1305     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000336 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 2.93e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00223 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.68     |\n",
      "|    value_loss         | 1.12e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 8.64e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.86     |\n",
      "|    value_loss         | 1.54e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1307      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -5.55e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.07      |\n",
      "|    value_loss         | 7.31e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1308      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -3.16e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.28      |\n",
      "|    value_loss         | 2.19e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1307      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -1.05e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.52      |\n",
      "|    value_loss         | 2.16e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1307      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.000835 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.76      |\n",
      "|    value_loss         | 1.47e-07  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1308      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000124 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.03      |\n",
      "|    value_loss         | 2.48e-09  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-08T22:25:00.000000000 to  2022-12-11T22:25:00.000000000\n",
      "A2C Sharpe Ratio:  -0.16219049572507158\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_14688_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1752        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00139012 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1640         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057099643 |\n",
      "|    clip_fraction        | 0.0584       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0267      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00467     |\n",
      "|    reward               | -0.000882941 |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.000182     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1591        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004948292 |\n",
      "|    clip_fraction        | 0.049       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    reward               | -0.00150718 |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 0.000132    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1576        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010681348 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.941       |\n",
      "|    value_loss           | 0.000112    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1567        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013992414 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0318     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    reward               | -0.00156448 |\n",
      "|    std                  | 0.905       |\n",
      "|    value_loss           | 6.6e-05     |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-12-08T22:25:00.000000000 to  2022-12-11T22:25:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2963538925574627\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_14688_1\n",
      "======DDPG Validation from:  2022-12-08T22:25:00.000000000 to  2022-12-11T22:25:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-11T22:25:00.000000000\n",
      "======Trading from:  2022-12-11T22:25:00.000000000 to  2022-12-12T22:25:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-11T22:25:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_14976_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1220      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000124 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 8.88e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1269      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -4.22e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 1.65e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00657  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 1.91e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1294     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0048   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.38     |\n",
      "|    value_loss         | 7.78e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1300     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.0029  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 2e-06    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 2.29e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 2.47e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1309     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00086 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 2.16e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000724 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 1.96e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000131 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 5.28e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.000531 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.06     |\n",
      "|    value_loss         | 9.47e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.000798 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 1.72e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1318      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -4.39e-10 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.36      |\n",
      "|    value_loss         | 2.31e-20  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000603 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.53     |\n",
      "|    value_loss         | 1.03e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 2.79e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.7      |\n",
      "|    value_loss         | 1.54e-14 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000822 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.9      |\n",
      "|    value_loss         | 1.37e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 2.04e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.11     |\n",
      "|    value_loss         | 4.33e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1318      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -7.44e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.33      |\n",
      "|    value_loss         | 1.21e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00104 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.56     |\n",
      "|    value_loss         | 1.38e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1320     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.00343  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.81     |\n",
      "|    value_loss         | 1.74e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.82     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.82e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.08      |\n",
      "|    value_loss         | 4.72e-11  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-11T22:25:00.000000000 to  2022-12-12T22:25:00.000000000\n",
      "A2C Sharpe Ratio:  -0.44246958709338846\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_14976_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1776       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 3.1908e-05 |\n",
      "-----------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1649          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0047979695  |\n",
      "|    clip_fraction        | 0.0195        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00776      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0022       |\n",
      "|    reward               | -0.0004900845 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 9e-05         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1609          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.012480276   |\n",
      "|    clip_fraction        | 0.137         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0272       |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0154       |\n",
      "|    reward               | -0.0003970668 |\n",
      "|    std                  | 0.954         |\n",
      "|    value_loss           | 4.7e-05       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1594        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011226086 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0402     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.922       |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1587         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.013355723  |\n",
      "|    clip_fraction        | 0.163        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0445      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0238      |\n",
      "|    reward               | -0.000627592 |\n",
      "|    std                  | 0.888        |\n",
      "|    value_loss           | 1.55e-05     |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-12-11T22:25:00.000000000 to  2022-12-12T22:25:00.000000000\n",
      "PPO Sharpe Ratio:  -1.1186697851394438\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_14976_1\n",
      "======DDPG Validation from:  2022-12-11T22:25:00.000000000 to  2022-12-12T22:25:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-12T22:25:00.000000000\n",
      "======Trading from:  2022-12-12T22:25:00.000000000 to  2022-12-13T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-12T22:25:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_15264_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1267     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.51    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.0022   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 4.52e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1293     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.00305  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 3.19e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1304     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00435  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 1.15e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.00205  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 1.77e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000653 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 2.28e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0024  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 3.22e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000319 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.66     |\n",
      "|    value_loss         | 3.42e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1313      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.21e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 4.68e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1315      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -4.81e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 4.13e-10  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1317      |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -6.43e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 1.1e-13   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 4.72e-09 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 1.24e-16 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.26     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -0.00142  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.33      |\n",
      "|    value_loss         | 5.24e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1321     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.34    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -1.3e-10 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 1.79e-16 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1322        |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.41       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 0.0013      |\n",
      "|    reward             | -0.00170288 |\n",
      "|    std                | 2.68        |\n",
      "|    value_loss         | 4.53e-07    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1322     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -8.7e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.87     |\n",
      "|    value_loss         | 1.37e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1323     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.00011 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 2.27e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1268      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.61     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.0043    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.29      |\n",
      "|    value_loss         | 2.88e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1270      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 6.27e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.53      |\n",
      "|    value_loss         | 8.12e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1274      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.75     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.000685 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.79      |\n",
      "|    value_loss         | 7.16e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1276     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.00234 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.06     |\n",
      "|    value_loss         | 7.19e-07 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-12T22:25:00.000000000 to  2022-12-13T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.25682101045239963\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_15264_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1776         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0002553774 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1655         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002565933  |\n",
      "|    clip_fraction        | 0.000586     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0166      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | 6.77e-05     |\n",
      "|    reward               | 0.0023179154 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00128      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1615        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006265831 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.019      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00236    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.000241    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1589        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008517824 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00931    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00893    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 0.000123    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1583          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.014139177   |\n",
      "|    clip_fraction        | 0.153         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0451       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0181       |\n",
      "|    reward               | -0.0007196608 |\n",
      "|    std                  | 0.942         |\n",
      "|    value_loss           | 7.99e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-12-12T22:25:00.000000000 to  2022-12-13T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2983948246867236\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_15264_1\n",
      "======DDPG Validation from:  2022-12-12T22:25:00.000000000 to  2022-12-13T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-13T22:30:00.000000000\n",
      "======Trading from:  2022-12-13T22:30:00.000000000 to  2022-12-14T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-13T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_15552_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1269       |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.48      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | 0.0331     |\n",
      "|    reward             | 0.04152001 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 0.00171    |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1291     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0036   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 8.47e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1303          |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.52         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 0.000456      |\n",
      "|    reward             | -0.0017168608 |\n",
      "|    std                | 1.11          |\n",
      "|    value_loss         | 1.27e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1307          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.56         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.00165      |\n",
      "|    reward             | -0.0007385016 |\n",
      "|    std                | 1.15          |\n",
      "|    value_loss         | 1.17e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1310         |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.61        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 2.67e-06     |\n",
      "|    reward             | -4.70865e-05 |\n",
      "|    std                | 1.21         |\n",
      "|    value_loss         | 5.64e-12     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.0047   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 7.67e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000171 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 1.47e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00783 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 1.88e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1315     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.00313 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 2.62e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1316     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 9.89e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 4.26e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.00526 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 7.77e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1314      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -5.56e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 1.12e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1315      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.000392  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.07      |\n",
      "|    value_loss         | 4.55e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 1.19e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 3.15e-11 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1317     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.28    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.00243  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.37     |\n",
      "|    value_loss         | 1.55e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1318     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.35    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00108  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.53     |\n",
      "|    value_loss         | 2.39e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1318      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -4.24e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.71      |\n",
      "|    value_loss         | 1.55e-16  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1319     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.48    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00035 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.9      |\n",
      "|    value_loss         | 2.96e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -9.53e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.1       |\n",
      "|    value_loss         | 2.37e-09  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1320      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.62     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000434 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.32      |\n",
      "|    value_loss         | 3.56e-08  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-13T22:30:00.000000000 to  2022-12-14T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.2795647064607888\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_15552_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1774        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 7.21464e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1643         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021015028 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0219      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000114    |\n",
      "|    reward               | 0.023485156  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000205     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1612         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010967536 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.216       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    reward               | 0.0007779692 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00756      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1597         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071266247 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.495       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0342      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00886     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00705      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1588          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0008037343  |\n",
      "|    clip_fraction        | 0.00361       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.835         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0127       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -7.99e-05     |\n",
      "|    reward               | -0.0018008432 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.000862      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-12-13T22:30:00.000000000 to  2022-12-14T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -0.30074313355306437\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_15552_1\n",
      "======DDPG Validation from:  2022-12-13T22:30:00.000000000 to  2022-12-14T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-14T22:30:00.000000000\n",
      "======Trading from:  2022-12-14T22:30:00.000000000 to  2022-12-15T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-14T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_15840_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1255      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -2.86     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.33     |\n",
      "|    reward             | 0.0550142 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 0.0465    |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1281         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | -2.23        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 0.0176       |\n",
      "|    reward             | -0.049959388 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 0.00489      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1292       |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.49      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 0.0294     |\n",
      "|    reward             | -0.0070202 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.00103    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1297       |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -9.08      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 0.3        |\n",
      "|    reward             | -0.0044583 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.0141     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1301       |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | -0.111     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -0.0235    |\n",
      "|    reward             | -0.0069861 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 0.000624   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1303        |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.0123     |\n",
      "|    reward             | -0.03954147 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.000142    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1305      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.00226   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 2.34e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1306     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.17     |\n",
      "|    value_loss         | 3.83e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00221  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 1.93e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1307     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.000265 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 5.94e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 5.06e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 1.55e-13 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1308     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 6.6e-09  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.41     |\n",
      "|    value_loss         | 1.08e-16 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 1309        |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -0.000194   |\n",
      "|    reward             | -0.00155096 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 9.96e-09    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1310     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.9     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00344 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.62     |\n",
      "|    value_loss         | 2.21e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1310      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.07e-05  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.73      |\n",
      "|    value_loss         | 8.5e-11   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1311     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.00355 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.86     |\n",
      "|    value_loss         | 3.38e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1312      |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -1.71e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.99      |\n",
      "|    value_loss         | 8.89e-19  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1312     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.00128  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 2.72e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.24    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.00334 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.28     |\n",
      "|    value_loss         | 3.24e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1313     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.00234 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.45     |\n",
      "|    value_loss         | 1.62e-06 |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-14T22:30:00.000000000 to  2022-12-15T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  0.0\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_15840_1\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 1752          |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 1             |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0001899202 |\n",
      "--------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1642          |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053082785 |\n",
      "|    clip_fraction        | 0.000684      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0176       |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000518     |\n",
      "|    reward               | 0.002996978   |\n",
      "|    std                  | 0.983         |\n",
      "|    value_loss           | 0.000135      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1610         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077186767 |\n",
      "|    clip_fraction        | 0.0896       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0298      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00567     |\n",
      "|    reward               | -0.000716949 |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 0.000611     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1595          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008680972   |\n",
      "|    clip_fraction        | 0.069         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.000836      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00722      |\n",
      "|    reward               | -0.0005715426 |\n",
      "|    std                  | 0.951         |\n",
      "|    value_loss           | 4.81e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1586          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.011691194   |\n",
      "|    clip_fraction        | 0.138         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0384       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.0183       |\n",
      "|    reward               | -0.0014661872 |\n",
      "|    std                  | 0.92          |\n",
      "|    value_loss           | 4.39e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-12-14T22:30:00.000000000 to  2022-12-15T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -1.2388589106195913\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_15840_1\n",
      "======DDPG Validation from:  2022-12-14T22:30:00.000000000 to  2022-12-15T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-15T22:30:00.000000000\n",
      "======Trading from:  2022-12-15T22:30:00.000000000 to  2022-12-18T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-15T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_16128_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1244         |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.5         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -0.00394     |\n",
      "|    reward             | -0.000285248 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 2.71e-06     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 1279       |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.000966  |\n",
      "|    reward             | -0.0023443 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 5.59e-07   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1290     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.65    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.00355  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.26     |\n",
      "|    value_loss         | 3.83e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 1296          |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.72         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.00418      |\n",
      "|    reward             | -0.0012637856 |\n",
      "|    std                | 1.35          |\n",
      "|    value_loss         | 4.33e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00171 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 1.28e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1301     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00313  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.54     |\n",
      "|    value_loss         | 5.45e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1297      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.00118   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 5.2e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1290     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00654 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.77     |\n",
      "|    value_loss         | 1.06e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000293 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 2.37e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1277     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 8.37e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.03     |\n",
      "|    value_loss         | 2.06e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1278      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.03e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 1.34e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1280      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.00329   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.33      |\n",
      "|    value_loss         | 2.34e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1281     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.33    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 3.38e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.5      |\n",
      "|    value_loss         | 2.01e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1283      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000608 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 8.59e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1284      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.47     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 1.59e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.87      |\n",
      "|    value_loss         | 2.98e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1286      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.08e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 2.47e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -2.5e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.28     |\n",
      "|    value_loss         | 1.51e-12 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1288      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.19e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.52      |\n",
      "|    value_loss         | 3.64e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000278 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.77     |\n",
      "|    value_loss         | 1.71e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1288      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -3.99e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.04      |\n",
      "|    value_loss         | 3.15e-12  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-15T22:30:00.000000000 to  2022-12-18T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.34531086634320846\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_16128_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1731         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0008025774 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1619        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003570884 |\n",
      "|    clip_fraction        | 0.0021      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.000119    |\n",
      "|    reward               | 0.02659769  |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.0016      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1580         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036646961 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -12.8        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00736     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    reward               | -0.00036558  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.0273       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1565        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004674941 |\n",
      "|    clip_fraction        | 0.0314      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.22       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00966    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    reward               | 0.016667072 |\n",
      "|    std                  | 0.991       |\n",
      "|    value_loss           | 0.00813     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1553         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069674095 |\n",
      "|    clip_fraction        | 0.051        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.217       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0201      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00434     |\n",
      "|    reward               | -0.0034608   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.00484      |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-12-15T22:30:00.000000000 to  2022-12-18T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -0.4333720725718568\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_16128_1\n",
      "======DDPG Validation from:  2022-12-15T22:30:00.000000000 to  2022-12-18T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-18T22:30:00.000000000\n",
      "======Trading from:  2022-12-18T22:30:00.000000000 to  2022-12-19T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-18T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_16416_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1242     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00377 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 8.33e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1274      |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.000737 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 2.75e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1283     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00174 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 1.61e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.0005  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 1.68e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1293      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -1.89e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 1.77e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.002   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 1.03e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00239 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 2.42e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1296      |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 3.32e-06  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.83      |\n",
      "|    value_loss         | 4.94e-12  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1297     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000526 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.96     |\n",
      "|    value_loss         | 7.56e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.0015  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.1      |\n",
      "|    value_loss         | 5.09e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1295     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 4.57e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.25     |\n",
      "|    value_loss         | 7.1e-12  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1296     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 3.68e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.4      |\n",
      "|    value_loss         | 3.5e-12  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1297      |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.36     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -7.72e-09 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.57      |\n",
      "|    value_loss         | 4.96e-18  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1298      |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -2.26e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.76      |\n",
      "|    value_loss         | 1.5e-10   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1298      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -1.28e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.96      |\n",
      "|    value_loss         | 3.84e-11  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1297      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.000171 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.17      |\n",
      "|    value_loss         | 6.67e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1298     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00248  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.39     |\n",
      "|    value_loss         | 1.11e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1298      |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.71     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.74e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.63      |\n",
      "|    value_loss         | 1.01e-13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1299      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.78     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -8.89e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.89      |\n",
      "|    value_loss         | 1.05e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1299     |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 7.8e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.16     |\n",
      "|    value_loss         | 1.3e-09  |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2022-12-18T22:30:00.000000000 to  2022-12-19T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.32761779780585415\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_16416_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 1727         |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 1            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0022665565 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1620        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010947756 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0334     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | 0.02699575  |\n",
      "|    std                  | 0.984       |\n",
      "|    value_loss           | 0.00531     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1575          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.006817814   |\n",
      "|    clip_fraction        | 0.0894        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -4.4          |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.000251      |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00886      |\n",
      "|    reward               | -0.0003372876 |\n",
      "|    std                  | 0.985         |\n",
      "|    value_loss           | 0.0305        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1560         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007905435  |\n",
      "|    clip_fraction        | 0.0901       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.223       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0204      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00806     |\n",
      "|    reward               | 0.0004103272 |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 0.00564      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1551        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001337996 |\n",
      "|    clip_fraction        | 0.0022      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.0571      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000287   |\n",
      "|    reward               | -0.00172168 |\n",
      "|    std                  | 0.962       |\n",
      "|    value_loss           | 0.000513    |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-12-18T22:30:00.000000000 to  2022-12-19T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -0.8424077192976789\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_16416_1\n",
      "======DDPG Validation from:  2022-12-18T22:30:00.000000000 to  2022-12-19T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-19T22:30:00.000000000\n",
      "======Trading from:  2022-12-19T22:30:00.000000000 to  2022-12-20T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-19T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_16704_1\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1236     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 0        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.00119 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 6.52e-07 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1267         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.61        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | -0.00631     |\n",
      "|    reward             | -0.000539189 |\n",
      "|    std                | 1.21         |\n",
      "|    value_loss         | 7.11e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1279     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.00903 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.3      |\n",
      "|    value_loss         | 2.35e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1285     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00107 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.4      |\n",
      "|    value_loss         | 5.77e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 2.07e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.5      |\n",
      "|    value_loss         | 1.43e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1286     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00136  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.6      |\n",
      "|    value_loss         | 8.79e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1288      |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.96     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000666 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 2.06e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1290     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.03    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.00155 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.84     |\n",
      "|    value_loss         | 1.03e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1290      |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.000736 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.96      |\n",
      "|    value_loss         | 8.68e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1291     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0018   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.1      |\n",
      "|    value_loss         | 1.16e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1289      |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.23     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.00223   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.25      |\n",
      "|    value_loss         | 9.96e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.3     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00693 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.41     |\n",
      "|    value_loss         | 1.1e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1290     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 2.58e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.58     |\n",
      "|    value_loss         | 2.01e-10 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1291     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00606 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.76     |\n",
      "|    value_loss         | 5.79e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1291      |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.5      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -0.00547  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.96      |\n",
      "|    value_loss         | 7.36e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1290      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -6.97e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.17      |\n",
      "|    value_loss         | 9.58e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.64    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00014 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.39     |\n",
      "|    value_loss         | 3.67e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.000133 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.63     |\n",
      "|    value_loss         | 3.5e-09  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1290      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -8.24e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.87      |\n",
      "|    value_loss         | 6.08e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1288      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.84     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 1.5e-07   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.14      |\n",
      "|    value_loss         | 7.16e-15  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-19T22:30:00.000000000 to  2022-12-20T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.456244257261001\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_16704_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1728        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00011572 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1614         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062108054 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0117      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    reward               | 0.00029335   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000755     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1577          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0072683813  |\n",
      "|    clip_fraction        | 0.0811        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.034        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00468      |\n",
      "|    reward               | -0.0011240414 |\n",
      "|    std                  | 0.99          |\n",
      "|    value_loss           | 0.000417      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1561         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.009817405  |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0292      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0105      |\n",
      "|    reward               | -8.96217e-05 |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 0.000257     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1547       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00816274 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | -0.0395    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    reward               | 0.0        |\n",
      "|    std                  | 0.945      |\n",
      "|    value_loss           | 0.000188   |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-12-19T22:30:00.000000000 to  2022-12-20T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -0.3794163391220345\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_16704_1\n",
      "======DDPG Validation from:  2022-12-19T22:30:00.000000000 to  2022-12-20T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-20T22:30:00.000000000\n",
      "======Trading from:  2022-12-20T22:30:00.000000000 to  2022-12-21T22:30:00.000000000\n",
      "============================================\n",
      "turbulence_threshold:  11.285107730988505\n",
      "======Model training from:  2022-01-01 to  2022-12-20T22:30:00.000000000\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_16992_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1232      |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.49     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.000515 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 2.56e-07  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1264         |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.56        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 5.31e-09     |\n",
      "|    reward             | -0.000672649 |\n",
      "|    std                | 1.15         |\n",
      "|    value_loss         | 2.53e-17     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 1273         |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.63        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | -0.00652     |\n",
      "|    reward             | -0.001177848 |\n",
      "|    std                | 1.24         |\n",
      "|    value_loss         | 9.27e-06     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1279      |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.000311 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 6.94e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1281      |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.000936 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.42      |\n",
      "|    value_loss         | 2.94e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1284     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.00305 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 5.05e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1277     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.9     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.000388 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 4.85e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1279     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00359  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.74     |\n",
      "|    value_loss         | 5.91e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1282     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.00252  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.87     |\n",
      "|    value_loss         | 2.18e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1283     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00598 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 1.07e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1285     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00287  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 3.13e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1286      |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.25     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -5.03e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.29      |\n",
      "|    value_loss         | 7.72e-17  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1287     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.32    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.00361 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.45     |\n",
      "|    value_loss         | 3.56e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0013  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.63     |\n",
      "|    value_loss         | 4.58e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1289     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 1.42e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.82     |\n",
      "|    value_loss         | 5.05e-16 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1289      |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 0.000792  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.02      |\n",
      "|    value_loss         | 1.04e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.000419 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.24     |\n",
      "|    value_loss         | 2.36e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 1288     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -1.6e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.47     |\n",
      "|    value_loss         | 4.41e-11 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1289      |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.73     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -2.01e-08 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.72      |\n",
      "|    value_loss         | 6.32e-17  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 1290      |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -2.14e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.98      |\n",
      "|    value_loss         | 6.33e-15  |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-12-20T22:30:00.000000000 to  2022-12-21T22:30:00.000000000\n",
      "A2C Sharpe Ratio:  -0.28872511869434786\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_16992_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1713        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.00021742 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1606         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019665598 |\n",
      "|    clip_fraction        | 0.0172       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0208      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    reward               | 0.0010519435 |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.000119     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1569         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001400173 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    reward               | -0.00145578  |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 0.000424     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1553          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00066441356 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0115       |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000759     |\n",
      "|    reward               | -0.00128031   |\n",
      "|    std                  | 0.956         |\n",
      "|    value_loss           | 0.00014       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1545          |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.007645808   |\n",
      "|    clip_fraction        | 0.0638        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0221       |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00619      |\n",
      "|    reward               | -0.0005848128 |\n",
      "|    std                  | 0.937         |\n",
      "|    value_loss           | 7.44e-05      |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2022-12-20T22:30:00.000000000 to  2022-12-21T22:30:00.000000000\n",
      "PPO Sharpe Ratio:  -1.1865473330976604\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_16992_1\n",
      "======DDPG Validation from:  2022-12-20T22:30:00.000000000 to  2022-12-21T22:30:00.000000000\n",
      "======Best Model Retraining from:  2022-01-01 to  2022-12-21T22:30:00.000000000\n",
      "======Trading from:  2022-12-21T22:30:00.000000000 to  2022-12-22T22:30:00.000000000\n",
      "Ensemble Strategy took:  214.05480243762335  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>576</td>\n",
       "      <td>2022-10-02 21:00:00</td>\n",
       "      <td>2022-10-03 21:00:00</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.09580</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>864</td>\n",
       "      <td>2022-10-03 21:00:00</td>\n",
       "      <td>2022-10-04 21:00:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.32167</td>\n",
       "      <td>-0.93563</td>\n",
       "      <td>0.09717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1152</td>\n",
       "      <td>2022-10-04 21:00:00</td>\n",
       "      <td>2022-10-05 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.23446</td>\n",
       "      <td>-1.25447</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1440</td>\n",
       "      <td>2022-10-05 21:05:00</td>\n",
       "      <td>2022-10-06 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.38131</td>\n",
       "      <td>-0.32252</td>\n",
       "      <td>-0.23709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1728</td>\n",
       "      <td>2022-10-06 21:05:00</td>\n",
       "      <td>2022-10-09 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.16253</td>\n",
       "      <td>-1.06735</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016</td>\n",
       "      <td>2022-10-09 21:05:00</td>\n",
       "      <td>2022-10-10 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.23666</td>\n",
       "      <td>-1.22246</td>\n",
       "      <td>-0.02724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2304</td>\n",
       "      <td>2022-10-10 21:05:00</td>\n",
       "      <td>2022-10-11 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.23120</td>\n",
       "      <td>-1.08768</td>\n",
       "      <td>-0.07906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2592</td>\n",
       "      <td>2022-10-11 21:05:00</td>\n",
       "      <td>2022-10-12 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.24839</td>\n",
       "      <td>-0.96220</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2880</td>\n",
       "      <td>2022-10-12 21:05:00</td>\n",
       "      <td>2022-10-13 21:05:00</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.14702</td>\n",
       "      <td>-0.05756</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3168</td>\n",
       "      <td>2022-10-13 21:05:00</td>\n",
       "      <td>2022-10-16 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.26848</td>\n",
       "      <td>-0.15386</td>\n",
       "      <td>-0.08221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3456</td>\n",
       "      <td>2022-10-16 21:05:00</td>\n",
       "      <td>2022-10-17 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.16681</td>\n",
       "      <td>-0.94521</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3744</td>\n",
       "      <td>2022-10-17 21:05:00</td>\n",
       "      <td>2022-10-18 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.14725</td>\n",
       "      <td>-1.17106</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4032</td>\n",
       "      <td>2022-10-18 21:05:00</td>\n",
       "      <td>2022-10-19 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.38845</td>\n",
       "      <td>-1.09954</td>\n",
       "      <td>-0.13679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4320</td>\n",
       "      <td>2022-10-19 21:05:00</td>\n",
       "      <td>2022-10-20 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.04207</td>\n",
       "      <td>-1.07180</td>\n",
       "      <td>-0.03702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4608</td>\n",
       "      <td>2022-10-20 21:05:00</td>\n",
       "      <td>2022-10-23 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.24047</td>\n",
       "      <td>-0.97946</td>\n",
       "      <td>0.06564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4896</td>\n",
       "      <td>2022-10-23 21:05:00</td>\n",
       "      <td>2022-10-24 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.13397</td>\n",
       "      <td>-0.36761</td>\n",
       "      <td>-0.09645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5184</td>\n",
       "      <td>2022-10-24 21:05:00</td>\n",
       "      <td>2022-10-25 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.28921</td>\n",
       "      <td>-1.38887</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5472</td>\n",
       "      <td>2022-10-25 21:05:00</td>\n",
       "      <td>2022-10-26 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.60523</td>\n",
       "      <td>-1.33311</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5760</td>\n",
       "      <td>2022-10-26 21:05:00</td>\n",
       "      <td>2022-10-27 21:05:00</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.17758</td>\n",
       "      <td>-1.06481</td>\n",
       "      <td>-0.17867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6048</td>\n",
       "      <td>2022-10-27 21:05:00</td>\n",
       "      <td>2022-10-30 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.57524</td>\n",
       "      <td>-1.30155</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6336</td>\n",
       "      <td>2022-10-30 21:05:00</td>\n",
       "      <td>2022-10-31 21:05:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.29872</td>\n",
       "      <td>-0.32779</td>\n",
       "      <td>-0.17032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6624</td>\n",
       "      <td>2022-10-31 21:05:00</td>\n",
       "      <td>2022-11-01 21:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.28311</td>\n",
       "      <td>-1.32448</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6912</td>\n",
       "      <td>2022-11-01 21:10:00</td>\n",
       "      <td>2022-11-02 21:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.25325</td>\n",
       "      <td>-0.33990</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7200</td>\n",
       "      <td>2022-11-02 21:10:00</td>\n",
       "      <td>2022-11-03 21:10:00</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.32598</td>\n",
       "      <td>-1.43404</td>\n",
       "      <td>-0.34625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7488</td>\n",
       "      <td>2022-11-03 21:10:00</td>\n",
       "      <td>2022-11-06 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.25378</td>\n",
       "      <td>-0.10825</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7776</td>\n",
       "      <td>2022-11-06 22:10:00</td>\n",
       "      <td>2022-11-07 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.20760</td>\n",
       "      <td>-1.01829</td>\n",
       "      <td>0.12139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8064</td>\n",
       "      <td>2022-11-07 22:10:00</td>\n",
       "      <td>2022-11-08 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.37496</td>\n",
       "      <td>-1.62530</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8352</td>\n",
       "      <td>2022-11-08 22:10:00</td>\n",
       "      <td>2022-11-09 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.34717</td>\n",
       "      <td>-0.35681</td>\n",
       "      <td>-0.20361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8640</td>\n",
       "      <td>2022-11-09 22:10:00</td>\n",
       "      <td>2022-11-10 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.32308</td>\n",
       "      <td>-1.21486</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8928</td>\n",
       "      <td>2022-11-10 22:10:00</td>\n",
       "      <td>2022-11-13 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.37228</td>\n",
       "      <td>-1.34755</td>\n",
       "      <td>-0.17403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>9216</td>\n",
       "      <td>2022-11-13 22:10:00</td>\n",
       "      <td>2022-11-14 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.28149</td>\n",
       "      <td>-1.09623</td>\n",
       "      <td>-0.08001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>9504</td>\n",
       "      <td>2022-11-14 22:10:00</td>\n",
       "      <td>2022-11-15 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.34718</td>\n",
       "      <td>-0.68060</td>\n",
       "      <td>-0.21718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9792</td>\n",
       "      <td>2022-11-15 22:10:00</td>\n",
       "      <td>2022-11-16 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.06366</td>\n",
       "      <td>-1.34154</td>\n",
       "      <td>0.07288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10080</td>\n",
       "      <td>2022-11-16 22:10:00</td>\n",
       "      <td>2022-11-17 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.24177</td>\n",
       "      <td>-0.29274</td>\n",
       "      <td>-0.02192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10368</td>\n",
       "      <td>2022-11-17 22:10:00</td>\n",
       "      <td>2022-11-20 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.22199</td>\n",
       "      <td>-1.13266</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10656</td>\n",
       "      <td>2022-11-20 22:10:00</td>\n",
       "      <td>2022-11-21 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.53789</td>\n",
       "      <td>-0.97484</td>\n",
       "      <td>-0.03407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10944</td>\n",
       "      <td>2022-11-21 22:10:00</td>\n",
       "      <td>2022-11-22 22:10:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.20535</td>\n",
       "      <td>-1.34195</td>\n",
       "      <td>-0.07120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11232</td>\n",
       "      <td>2022-11-22 22:10:00</td>\n",
       "      <td>2022-11-23 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.16688</td>\n",
       "      <td>-0.17736</td>\n",
       "      <td>-0.13377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>11520</td>\n",
       "      <td>2022-11-23 22:20:00</td>\n",
       "      <td>2022-11-24 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.36108</td>\n",
       "      <td>-1.44335</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>11808</td>\n",
       "      <td>2022-11-24 22:20:00</td>\n",
       "      <td>2022-11-27 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.24588</td>\n",
       "      <td>-1.19100</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12096</td>\n",
       "      <td>2022-11-27 22:20:00</td>\n",
       "      <td>2022-11-28 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.31945</td>\n",
       "      <td>-1.44961</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>12384</td>\n",
       "      <td>2022-11-28 22:20:00</td>\n",
       "      <td>2022-11-29 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.20216</td>\n",
       "      <td>-0.31284</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12672</td>\n",
       "      <td>2022-11-29 22:20:00</td>\n",
       "      <td>2022-11-30 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.08609</td>\n",
       "      <td>-1.48737</td>\n",
       "      <td>-0.07593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>12960</td>\n",
       "      <td>2022-11-30 22:20:00</td>\n",
       "      <td>2022-12-01 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.30717</td>\n",
       "      <td>-0.71855</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>13248</td>\n",
       "      <td>2022-12-01 22:20:00</td>\n",
       "      <td>2022-12-04 22:20:00</td>\n",
       "      <td>PPO</td>\n",
       "      <td>-0.27361</td>\n",
       "      <td>-0.19788</td>\n",
       "      <td>-0.21288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>13536</td>\n",
       "      <td>2022-12-04 22:20:00</td>\n",
       "      <td>2022-12-05 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.25519</td>\n",
       "      <td>-0.36272</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13824</td>\n",
       "      <td>2022-12-05 22:20:00</td>\n",
       "      <td>2022-12-06 22:20:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.33195</td>\n",
       "      <td>-1.09363</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14112</td>\n",
       "      <td>2022-12-06 22:20:00</td>\n",
       "      <td>2022-12-07 22:25:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.26158</td>\n",
       "      <td>-0.10190</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14400</td>\n",
       "      <td>2022-12-07 22:25:00</td>\n",
       "      <td>2022-12-08 22:25:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.30869</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>14688</td>\n",
       "      <td>2022-12-08 22:25:00</td>\n",
       "      <td>2022-12-11 22:25:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.16219</td>\n",
       "      <td>-1.29635</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>14976</td>\n",
       "      <td>2022-12-11 22:25:00</td>\n",
       "      <td>2022-12-12 22:25:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.44247</td>\n",
       "      <td>-1.11867</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>15264</td>\n",
       "      <td>2022-12-12 22:25:00</td>\n",
       "      <td>2022-12-13 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.25682</td>\n",
       "      <td>-1.29839</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>15552</td>\n",
       "      <td>2022-12-13 22:30:00</td>\n",
       "      <td>2022-12-14 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.27956</td>\n",
       "      <td>-0.30074</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>15840</td>\n",
       "      <td>2022-12-14 22:30:00</td>\n",
       "      <td>2022-12-15 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.23886</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>16128</td>\n",
       "      <td>2022-12-15 22:30:00</td>\n",
       "      <td>2022-12-18 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.34531</td>\n",
       "      <td>-0.43337</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>16416</td>\n",
       "      <td>2022-12-18 22:30:00</td>\n",
       "      <td>2022-12-19 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.32762</td>\n",
       "      <td>-0.84241</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>16704</td>\n",
       "      <td>2022-12-19 22:30:00</td>\n",
       "      <td>2022-12-20 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.45624</td>\n",
       "      <td>-0.37942</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>16992</td>\n",
       "      <td>2022-12-20 22:30:00</td>\n",
       "      <td>2022-12-21 22:30:00</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.28873</td>\n",
       "      <td>-1.18655</td>\n",
       "      <td>-0.12113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Iter           Val Start             Val End Model Used A2C Sharpe  \\\n",
       "0     576 2022-10-02 21:00:00 2022-10-03 21:00:00        PPO    0.00000   \n",
       "1     864 2022-10-03 21:00:00 2022-10-04 21:00:00       DDPG   -0.32167   \n",
       "2    1152 2022-10-04 21:00:00 2022-10-05 21:05:00       DDPG   -0.23446   \n",
       "3    1440 2022-10-05 21:05:00 2022-10-06 21:05:00       DDPG   -0.38131   \n",
       "4    1728 2022-10-06 21:05:00 2022-10-09 21:05:00       DDPG   -0.16253   \n",
       "5    2016 2022-10-09 21:05:00 2022-10-10 21:05:00       DDPG   -0.23666   \n",
       "6    2304 2022-10-10 21:05:00 2022-10-11 21:05:00       DDPG   -0.23120   \n",
       "7    2592 2022-10-11 21:05:00 2022-10-12 21:05:00       DDPG   -0.24839   \n",
       "8    2880 2022-10-12 21:05:00 2022-10-13 21:05:00        A2C    0.14702   \n",
       "9    3168 2022-10-13 21:05:00 2022-10-16 21:05:00       DDPG   -0.26848   \n",
       "10   3456 2022-10-16 21:05:00 2022-10-17 21:05:00       DDPG   -0.16681   \n",
       "11   3744 2022-10-17 21:05:00 2022-10-18 21:05:00       DDPG   -0.14725   \n",
       "12   4032 2022-10-18 21:05:00 2022-10-19 21:05:00       DDPG   -0.38845   \n",
       "13   4320 2022-10-19 21:05:00 2022-10-20 21:05:00       DDPG   -0.04207   \n",
       "14   4608 2022-10-20 21:05:00 2022-10-23 21:05:00       DDPG   -0.24047   \n",
       "15   4896 2022-10-23 21:05:00 2022-10-24 21:05:00       DDPG   -0.13397   \n",
       "16   5184 2022-10-24 21:05:00 2022-10-25 21:05:00       DDPG   -0.28921   \n",
       "17   5472 2022-10-25 21:05:00 2022-10-26 21:05:00       DDPG   -0.60523   \n",
       "18   5760 2022-10-26 21:05:00 2022-10-27 21:05:00        A2C   -0.17758   \n",
       "19   6048 2022-10-27 21:05:00 2022-10-30 21:05:00       DDPG   -0.57524   \n",
       "20   6336 2022-10-30 21:05:00 2022-10-31 21:05:00       DDPG   -0.29872   \n",
       "21   6624 2022-10-31 21:05:00 2022-11-01 21:10:00       DDPG   -0.28311   \n",
       "22   6912 2022-11-01 21:10:00 2022-11-02 21:10:00       DDPG   -0.25325   \n",
       "23   7200 2022-11-02 21:10:00 2022-11-03 21:10:00        A2C   -0.32598   \n",
       "24   7488 2022-11-03 21:10:00 2022-11-06 22:10:00       DDPG   -0.25378   \n",
       "25   7776 2022-11-06 22:10:00 2022-11-07 22:10:00       DDPG   -0.20760   \n",
       "26   8064 2022-11-07 22:10:00 2022-11-08 22:10:00       DDPG   -0.37496   \n",
       "27   8352 2022-11-08 22:10:00 2022-11-09 22:10:00       DDPG   -0.34717   \n",
       "28   8640 2022-11-09 22:10:00 2022-11-10 22:10:00       DDPG   -0.32308   \n",
       "29   8928 2022-11-10 22:10:00 2022-11-13 22:10:00       DDPG   -0.37228   \n",
       "30   9216 2022-11-13 22:10:00 2022-11-14 22:10:00       DDPG   -0.28149   \n",
       "31   9504 2022-11-14 22:10:00 2022-11-15 22:10:00       DDPG   -0.34718   \n",
       "32   9792 2022-11-15 22:10:00 2022-11-16 22:10:00       DDPG    0.06366   \n",
       "33  10080 2022-11-16 22:10:00 2022-11-17 22:10:00       DDPG   -0.24177   \n",
       "34  10368 2022-11-17 22:10:00 2022-11-20 22:10:00       DDPG   -0.22199   \n",
       "35  10656 2022-11-20 22:10:00 2022-11-21 22:10:00       DDPG   -0.53789   \n",
       "36  10944 2022-11-21 22:10:00 2022-11-22 22:10:00       DDPG   -0.20535   \n",
       "37  11232 2022-11-22 22:10:00 2022-11-23 22:20:00       DDPG   -0.16688   \n",
       "38  11520 2022-11-23 22:20:00 2022-11-24 22:20:00       DDPG   -0.36108   \n",
       "39  11808 2022-11-24 22:20:00 2022-11-27 22:20:00       DDPG   -0.24588   \n",
       "40  12096 2022-11-27 22:20:00 2022-11-28 22:20:00       DDPG   -0.31945   \n",
       "41  12384 2022-11-28 22:20:00 2022-11-29 22:20:00       DDPG   -0.20216   \n",
       "42  12672 2022-11-29 22:20:00 2022-11-30 22:20:00       DDPG   -0.08609   \n",
       "43  12960 2022-11-30 22:20:00 2022-12-01 22:20:00       DDPG   -0.30717   \n",
       "44  13248 2022-12-01 22:20:00 2022-12-04 22:20:00        PPO   -0.27361   \n",
       "45  13536 2022-12-04 22:20:00 2022-12-05 22:20:00       DDPG   -0.25519   \n",
       "46  13824 2022-12-05 22:20:00 2022-12-06 22:20:00       DDPG   -0.33195   \n",
       "47  14112 2022-12-06 22:20:00 2022-12-07 22:25:00       DDPG   -0.26158   \n",
       "48  14400 2022-12-07 22:25:00 2022-12-08 22:25:00       DDPG    0.00000   \n",
       "49  14688 2022-12-08 22:25:00 2022-12-11 22:25:00       DDPG   -0.16219   \n",
       "50  14976 2022-12-11 22:25:00 2022-12-12 22:25:00       DDPG   -0.44247   \n",
       "51  15264 2022-12-12 22:25:00 2022-12-13 22:30:00       DDPG   -0.25682   \n",
       "52  15552 2022-12-13 22:30:00 2022-12-14 22:30:00       DDPG   -0.27956   \n",
       "53  15840 2022-12-14 22:30:00 2022-12-15 22:30:00       DDPG    0.00000   \n",
       "54  16128 2022-12-15 22:30:00 2022-12-18 22:30:00       DDPG   -0.34531   \n",
       "55  16416 2022-12-18 22:30:00 2022-12-19 22:30:00       DDPG   -0.32762   \n",
       "56  16704 2022-12-19 22:30:00 2022-12-20 22:30:00       DDPG   -0.45624   \n",
       "57  16992 2022-12-20 22:30:00 2022-12-21 22:30:00       DDPG   -0.28873   \n",
       "\n",
       "   PPO Sharpe DDPG Sharpe  \n",
       "0     0.09580     0.00000  \n",
       "1    -0.93563     0.09717  \n",
       "2    -1.25447     0.00000  \n",
       "3    -0.32252    -0.23709  \n",
       "4    -1.06735     0.00000  \n",
       "5    -1.22246    -0.02724  \n",
       "6    -1.08768    -0.07906  \n",
       "7    -0.96220     0.00000  \n",
       "8    -0.05756     0.00000  \n",
       "9    -0.15386    -0.08221  \n",
       "10   -0.94521     0.00000  \n",
       "11   -1.17106     0.00000  \n",
       "12   -1.09954    -0.13679  \n",
       "13   -1.07180    -0.03702  \n",
       "14   -0.97946     0.06564  \n",
       "15   -0.36761    -0.09645  \n",
       "16   -1.38887     0.00000  \n",
       "17   -1.33311     0.00000  \n",
       "18   -1.06481    -0.17867  \n",
       "19   -1.30155     0.00000  \n",
       "20   -0.32779    -0.17032  \n",
       "21   -1.32448     0.00000  \n",
       "22   -0.33990     0.00000  \n",
       "23   -1.43404    -0.34625  \n",
       "24   -0.10825     0.00000  \n",
       "25   -1.01829     0.12139  \n",
       "26   -1.62530     0.00000  \n",
       "27   -0.35681    -0.20361  \n",
       "28   -1.21486     0.00000  \n",
       "29   -1.34755    -0.17403  \n",
       "30   -1.09623    -0.08001  \n",
       "31   -0.68060    -0.21718  \n",
       "32   -1.34154     0.07288  \n",
       "33   -0.29274    -0.02192  \n",
       "34   -1.13266     0.00000  \n",
       "35   -0.97484    -0.03407  \n",
       "36   -1.34195    -0.07120  \n",
       "37   -0.17736    -0.13377  \n",
       "38   -1.44335     0.00000  \n",
       "39   -1.19100     0.00000  \n",
       "40   -1.44961     0.00000  \n",
       "41   -0.31284     0.00000  \n",
       "42   -1.48737    -0.07593  \n",
       "43   -0.71855     0.00000  \n",
       "44   -0.19788    -0.21288  \n",
       "45   -0.36272     0.00000  \n",
       "46   -1.09363     0.00000  \n",
       "47   -0.10190     0.00000  \n",
       "48   -1.30869     0.00000  \n",
       "49   -1.29635     0.00000  \n",
       "50   -1.11867     0.00000  \n",
       "51   -1.29839     0.00000  \n",
       "52   -0.30074     0.00000  \n",
       "53   -1.23886     0.00000  \n",
       "54   -0.43337     0.00000  \n",
       "55   -0.84241     0.00000  \n",
       "56   -0.37942     0.00000  \n",
       "57   -1.18655    -0.12113  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > '2022-10-01')&(processed.date <= '2022-12-25')].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  -0.25308573042113736\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = df_account_value.append(temp,ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.00115\n",
      "Cumulative returns    -0.07340\n",
      "Annual volatility      0.00450\n",
      "Sharpe ratio          -0.25309\n",
      "Calmar ratio          -0.01449\n",
      "Stability              0.86456\n",
      "Max drawdown          -0.07933\n",
      "Omega ratio            0.92795\n",
      "Sortino ratio         -0.36176\n",
      "Skew                       NaN\n",
      "Kurtosis                   NaN\n",
      "Tail ratio             0.95410\n",
      "Daily value at risk   -0.00057\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"True\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtest as bt\n",
    "\n",
    "env = test_env\n",
    "bt.backtest(agent, env)\n",
    "env.data['p'].iloc[env.lags:].value_counts()\n",
    "sum(env.data['p'].iloc[env.lags:].diff() != 0)\n",
    "\n",
    "(env.data[['r', 's']].iloc[env.lags:] * env.leverage).sum(\n",
    "        ).apply(np.exp)\n",
    "\n",
    "(env.data[['r', 's']].iloc[env.lags:] * env.leverage).sum(\n",
    "        ).apply(np.exp) - 1\n",
    "\n",
    "(env.data[['r', 's']].iloc[env.lags:] * env.leverage).cumsum(\n",
    "        ).apply(np.exp).plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpqoa\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import talib as ta\n",
    "\n",
    "class MLTrader(tpqoa.tpqoa):\n",
    "    def __init__(self, config_file, algorithm):\n",
    "        super(MLTrader, self).__init__(config_file)\n",
    "        self.model = algorithm['model'] \n",
    "        self.mu = algorithm['mu'] \n",
    "        self.std = algorithm['std'] \n",
    "        self.units = 100000 \n",
    "        self.position = 0 \n",
    "        self.bar = '5s' \n",
    "        self.window = 2 \n",
    "        self.lags = 10 \n",
    "        self.min_length = self.lags + self.window + 63\n",
    "        #self.features = algorithm['cols'] \n",
    "        self.raw_data = pd.DataFrame()        \n",
    "    def report_trade(self, pos, order):\n",
    "        out = '\\n\\n' + 80 * '=' + '\\n'\n",
    "        out += '*** GOING {} *** \\n'.format(pos) + '\\n'\n",
    "        out += str(order) + '\\n'\n",
    "        out += 80 * '=' + '\\n'\n",
    "        print(out)\n",
    "    def prepare_features(self): \n",
    "        # MOMENTUM\n",
    "        self.data['ADX'] = ta.ADX(self.data.high.values, self.data.low.values,self.data.close.values)  \n",
    "        self.data['ADXR'] = ta.ADXR(self.data.high.values, self.data.low.values,self.data.close.values)\n",
    "        self.data['APO'] = ta.APO(self.data.close.values) \n",
    "        self.data['AROONdown'],self.data['AROONup'] = ta.AROON(self.data.high.values, self.data.low.values)\n",
    "        self.data['AROONdmu'] = self.data['AROONdown'] - self.data['AROONup']        \n",
    "        self.data['AROONOSC'] = ta.AROONOSC(self.data.high.values, self.data.low.values)        \n",
    "        self.data['BOP'] = ta.BOP(self.data.open.values,self.data.high.values, self.data.low.values,self.data.close.values)     \n",
    "        self.data['CCI'] = ta.CCI(self.data.high.values, self.data.low.values,self.data.close.values)          \n",
    "        self.data['CMO'] = ta.CMO(self.data.close.values) \n",
    "        self.data['DX'] = ta.DX(self.data.high.values, self.data.low.values,self.data.close.values)     \n",
    "        self.data['macd'],self.data['macdsignal'],self.data['macdhist'] = ta.MACD(self.data.close.values)       \n",
    "        self.data['macdFIX'],self.data['macdsignalFIX'],self.data['macdhistFIX'] = ta.MACDFIX(self.data.close.values)   \n",
    "        self.data['MINUS_DI'] = ta.MINUS_DI(self.data.high.values, self.data.low.values,self.data.close.values)\n",
    "        self.data['MINUS_DM'] = ta.MINUS_DM(self.data.high.values, self.data.low.values) \n",
    "        self.data['MOM'] = ta.MOM(self.data.close.values) \n",
    "        self.data['PLUS_DI'] = ta.PLUS_DI(self.data.high.values, self.data.low.values,self.data.close.values)\n",
    "        self.data['PLUS_DM'] = ta.PLUS_DM(self.data.high.values, self.data.low.values)   \n",
    "        self.data['PPO'] = ta.PPO(self.data.close.values) \n",
    "        self.data['ROC'] = ta.ROC(self.data.close.values) \n",
    "        self.data['ROCP'] = ta.ROCP(self.data.close.values) \n",
    "        self.data['ROCR100'] = ta.ROCR100(self.data.close.values) \n",
    "        self.data['RSI'] = ta.RSI(self.data.close.values,14) \n",
    "        self.data['slk'],self.data['sld'] = ta.STOCH(self.data.high.values, self.data.low.values,self.data.close.values)\n",
    "        self.data['STOCHkmdSl'] = self.data['slk'] - self.data['sld'] \n",
    "        self.data['fastk'],self.data['fastd'] = ta.STOCHF(self.data.high.values, self.data.low.values,self.data.close.values)\n",
    "        self.data['STOCHkmdFast'] = self.data['fastk'] - self.data['fastd']       \n",
    "        self.data['slkRSI'],self.data['sldRSI'] = ta.STOCHRSI(self.data.close.values)\n",
    "        self.data['STOCKkmdRSI'] = self.data['slkRSI'] - self.data['sldRSI']        \n",
    "        self.data['TRIX'] = ta.TRIX(self.data.close.values)\n",
    "        self.data['ULTOSC'] = ta.ULTOSC(self.data.high.values, self.data.low.values,self.data.close.values)   \n",
    "        self.data['WILLR'] = ta.WILLR(self.data.high.values, self.data.low.values,self.data.close.values)           \n",
    "               \n",
    "\n",
    "        #VOLATILITY\n",
    "        self.data['ATR63'] = ta.ATR(self.data.high.values, self.data.low.values,self.data.close.values,63)     \n",
    "        self.data['ATR'] = ta.ATR(self.data.high.values, self.data.low.values,self.data.close.values)  \n",
    "        self.data['NATR'] = ta.NATR(self.data.high.values, self.data.low.values,self.data.close.values)         \n",
    "        \n",
    "        # CYCLES\n",
    "        self.data['HT_DCPERIOD'] = ta.HT_DCPERIOD(self.data.close.values) \n",
    "        self.data['HT_DCPHASE'] = ta.HT_DCPHASE(self.data.close.values)\n",
    "        self.data['inphase'],self.data['quadrature'] = ta.HT_PHASOR(self.data.close.values)\n",
    "        self.data['sine'],self.data['leadsine'] = ta.HT_SINE(self.data.close.values)\n",
    "        self.data['sls'] = self.data['sine'] - self.data['leadsine']\n",
    "        self.data['HT_TRENDMODE'] = ta.HT_TRENDMODE(self.data.close.values) \n",
    "        \n",
    "        # PRICE-ACTION\n",
    "        self.data['C/O'] = self.data.close / self.data.open -1\n",
    "        self.data['H/L'] = self.data.high / self.data.low -1\n",
    "        self.data['C/L'] = self.data.close / self.data.low -1\n",
    "        self.data['H/Lt'] = self.data.high / self.data.low.shift() -1\n",
    "        self.data['L/Lt'] = self.data.low / self.data.low.shift() -1\n",
    "        self.data['H/Ht2'] = self.data.high / self.data.high.shift(2) -1\n",
    "        self.data['H/Lt3'] = self.data.high / self.data.low.shift(3) -1\n",
    "        self.data['L/Lt'] = self.data.low / self.data.low.shift() -1\n",
    "        self.data['C/Ct2'] = self.data.close / self.data.close.shift(2) -1\n",
    "        self.data['C/Ct3'] = self.data.close / self.data.close.shift(3) -1\n",
    "        \n",
    "        self.data['returns'] = np.log(self.data['close'] / self.data['close'] .shift())\n",
    "        self.data['direction'] = np.where(self.data['returns'] > 0, 1, 0)\n",
    "        \n",
    "        self.data.dropna(inplace=True)\n",
    "        \n",
    "        self.cols = []\n",
    "        self.features = self.data.drop(['open', 'high', 'low', 'close'], axis=1).columns.tolist()\n",
    "        for f in self.features:\n",
    "            for lag in range(1, self.lags + 1):\n",
    "                col = f'{f}_lag_{lag}'\n",
    "                self.data[col] = self.data[f].shift(lag)\n",
    "                self.cols.append(col)\n",
    "        \n",
    "        # normalize data\n",
    "        self.data[self.features] -= self.mu\n",
    "        self.data[self.features] /= self.std\n",
    "        \n",
    "\n",
    "    def on_success(self, time, bid, ask):\n",
    "        print(self.ticks, 20 * ' ', end='\\r')\n",
    "        df = pd.DataFrame({'bid': float(bid), 'ask': float(ask)},\n",
    "        index=[pd.Timestamp(time).tz_localize(None)])\n",
    "        self.raw_data = self.raw_data.append(df)\n",
    "        self.data = self.raw_data.resample(\n",
    "        self.bar, label='right').last().ffill()\n",
    "        self.data = self.data.iloc[:-1]\n",
    "        if len(self.data) > self.min_length:\n",
    "            self.min_length += 1\n",
    "            self.data['close'] = (self.data['bid'] + self.data['ask']) / 2\n",
    "            self.data = self.data['close'].resample('10S').ohlc()\n",
    "            self.prepare_features()\n",
    "            features = self.data[self.cols].iloc[-1:,:].values.reshape(1, -1)\n",
    "            signal = self.model.predict(features)[0]\n",
    "    \n",
    "            if self.position in [0, -1] and signal == 1: # going long?\n",
    "                order = self.create_order(self.stream_instrument,\n",
    "                units=(1 - self.position) *\n",
    "                self.units,\n",
    "                uppress=True, ret=True)\n",
    "                self.report_trade('LONG', order)\n",
    "                self.position = 1\n",
    "            elif self.position in [0, 1] and signal == -1: # going short?\n",
    "                order = self.create_order(self.stream_instrument,\n",
    "                units=-(1 + self.position) *\n",
    "                self.units,\n",
    "                suppress=True, ret=True)\n",
    "                self.report_trade('SHORT', order)\n",
    "                self.position = -1\n",
    "\n",
    "# load persisted model\n",
    "algo = pickle.load(open('algorithm.pkl', 'rb'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # instantiate class\n",
    "    mlt = MLTrader('ONDdemo.cfg', algo) \n",
    "    mlt.stream_data('EUR_USD') # stop=10000\n",
    "    order = mlt.create_order(mlt.stream_instrument,\n",
    "    units=-mlt.position * mlt.units,\n",
    "    suppress=True, ret=True)\n",
    "    mlt.position = 0\n",
    "    mlt.report_trade('NEUTRAL', order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 18:24:45) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
